{"id":"test_2619","question":"when does a wrinkle in time come out in canada?","golden_answers":["March 9, 2018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does a wrinkle in time come out in canada?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["March 9, 2018"]},"style":"rule"},"extra_info":{"index":0,"split":"test"}}
{"id":"test_456","question":"what is the minimum wage in france per hour?","golden_answers":["11.16","\u20ac9.88 per hour."],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the minimum wage in france per hour?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["11.16","\u20ac9.88 per hour."]},"style":"rule"},"extra_info":{"index":1,"split":"test"}}
{"id":"test_102","question":"who plays the evil doctor in wonder woman?","golden_answers":["Elena Anaya","Spanish actress Elena Anaya"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the evil doctor in wonder woman?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Elena Anaya","Spanish actress Elena Anaya"]},"style":"rule"},"extra_info":{"index":2,"split":"test"}}
{"id":"test_3037","question":"who won big brother head of household canada?","golden_answers":["Kaela"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won big brother head of household canada?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kaela"]},"style":"rule"},"extra_info":{"index":3,"split":"test"}}
{"id":"test_1126","question":"when did end of the road come out?","golden_answers":["1992","June\u00a030,\u00a01992"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did end of the road come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1992","June\u00a030,\u00a01992"]},"style":"rule"},"extra_info":{"index":4,"split":"test"}}
{"id":"test_1003","question":"who is the founder of el pollo loco?","golden_answers":["Juan Francisco Ochoa"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the founder of el pollo loco?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Juan Francisco Ochoa"]},"style":"rule"},"extra_info":{"index":5,"split":"test"}}
{"id":"test_914","question":"what was the full name of the titanic?","golden_answers":["RMS Titanic"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the full name of the titanic?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["RMS Titanic"]},"style":"rule"},"extra_info":{"index":6,"split":"test"}}
{"id":"test_571","question":"who is playing halftime show super bowl 50?","golden_answers":["Beyonc\u00e9","Coldplay","Bruno Mars","the British rock group Coldplay"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is playing halftime show super bowl 50?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Beyonc\u00e9","Coldplay","Bruno Mars","the British rock group Coldplay"]},"style":"rule"},"extra_info":{"index":7,"split":"test"}}
{"id":"test_3016","question":"which president supported the creation of the environmental protection agency (epa)?","golden_answers":["Richard Nixon","President Richard Nixon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which president supported the creation of the environmental protection agency (epa)?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Richard Nixon","President Richard Nixon"]},"style":"rule"},"extra_info":{"index":8,"split":"test"}}
{"id":"test_419","question":"who was appointed to manage the national recovery administration?","golden_answers":["Hugh S. Johnson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was appointed to manage the national recovery administration?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Hugh S. Johnson"]},"style":"rule"},"extra_info":{"index":9,"split":"test"}}
{"id":"test_2771","question":"when did taylor swift's first album release?","golden_answers":["October 24, 2006","2005"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did taylor swift's first album release?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["October 24, 2006","2005"]},"style":"rule"},"extra_info":{"index":10,"split":"test"}}
{"id":"test_3033","question":"what's the population of prince edward island?","golden_answers":["142,907 residents","142,907"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what's the population of prince edward island?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["142,907 residents","142,907"]},"style":"rule"},"extra_info":{"index":11,"split":"test"}}
{"id":"test_2233","question":"what is the name of the restaurant in seinfeld?","golden_answers":["Monk\u2019s","Monk's Caf\u00e9"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the name of the restaurant in seinfeld?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Monk\u2019s","Monk's Caf\u00e9"]},"style":"rule"},"extra_info":{"index":12,"split":"test"}}
{"id":"test_356","question":"who plays red on orange is new black?","golden_answers":["\"Kate\" Mulgrew"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays red on orange is new black?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["\"Kate\" Mulgrew"]},"style":"rule"},"extra_info":{"index":13,"split":"test"}}
{"id":"test_2418","question":"the initial unification of upper and lower egypt took place during which period?","golden_answers":["3000 BC","c. 3000 BC"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the initial unification of upper and lower egypt took place during which period?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["3000 BC","c. 3000 BC"]},"style":"rule"},"extra_info":{"index":14,"split":"test"}}
{"id":"test_1728","question":"who talks for belle in beauty and the beast?","golden_answers":["Julie Nathanson","Jodi Benson","Paige O'Hara"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who talks for belle in beauty and the beast?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Julie Nathanson","Jodi Benson","Paige O'Hara"]},"style":"rule"},"extra_info":{"index":15,"split":"test"}}
{"id":"test_130","question":"total number of mna in pakistan national assembly?","golden_answers":["332 members","332"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: total number of mna in pakistan national assembly?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["332 members","332"]},"style":"rule"},"extra_info":{"index":16,"split":"test"}}
{"id":"test_122","question":"what is bermuda competing in the winter olympics?","golden_answers":["Cross-country skiing"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is bermuda competing in the winter olympics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Cross-country skiing"]},"style":"rule"},"extra_info":{"index":17,"split":"test"}}
{"id":"test_383","question":"how many episodes of touching evil are there?","golden_answers":["16"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes of touching evil are there?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["16"]},"style":"rule"},"extra_info":{"index":18,"split":"test"}}
{"id":"test_895","question":"sending money home to the native country is an example of?","golden_answers":["international capital flows","remittance"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: sending money home to the native country is an example of?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["international capital flows","remittance"]},"style":"rule"},"extra_info":{"index":19,"split":"test"}}
{"id":"test_952","question":"which is the world's largest company in terms of revenue?","golden_answers":["Walmart"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which is the world's largest company in terms of revenue?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Walmart"]},"style":"rule"},"extra_info":{"index":20,"split":"test"}}
{"id":"test_2069","question":"who won the most gold metals in olympics?","golden_answers":["American swimmer Michael Phelps","Michael Phelps"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the most gold metals in olympics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["American swimmer Michael Phelps","Michael Phelps"]},"style":"rule"},"extra_info":{"index":21,"split":"test"}}
{"id":"test_2465","question":"who owns the delano hotel in las vegas?","golden_answers":["MGM Resorts International"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who owns the delano hotel in las vegas?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["MGM Resorts International"]},"style":"rule"},"extra_info":{"index":22,"split":"test"}}
{"id":"test_108","question":"what is the final season of downton abbey?","golden_answers":["the sixth","sixth","six","the sixth series","Six"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the final season of downton abbey?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the sixth","sixth","six","the sixth series","Six"]},"style":"rule"},"extra_info":{"index":23,"split":"test"}}
{"id":"test_2298","question":"who plays the principal in santa clarita diet?","golden_answers":["Thomas Lennon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the principal in santa clarita diet?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Thomas Lennon"]},"style":"rule"},"extra_info":{"index":24,"split":"test"}}
{"id":"test_814","question":"when did under the cork tree come out?","golden_answers":["May 3, 2005"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did under the cork tree come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["May 3, 2005"]},"style":"rule"},"extra_info":{"index":25,"split":"test"}}
{"id":"test_2932","question":"when do you declare honors in contract bridge?","golden_answers":["any time after the auction"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when do you declare honors in contract bridge?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["any time after the auction"]},"style":"rule"},"extra_info":{"index":26,"split":"test"}}
{"id":"test_2661","question":"what were the two causes of the dust bowl?","golden_answers":["severe drought"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what were the two causes of the dust bowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["severe drought"]},"style":"rule"},"extra_info":{"index":27,"split":"test"}}
{"id":"test_2872","question":"who played taylor on the bold and beautiful?","golden_answers":["Hunter Tylo","Sherilyn Wolter","Sherilyn Wolter (1990)"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played taylor on the bold and beautiful?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Hunter Tylo","Sherilyn Wolter","Sherilyn Wolter (1990)"]},"style":"rule"},"extra_info":{"index":28,"split":"test"}}
{"id":"test_2232","question":"how many kilometers of great wall of china?","golden_answers":["8,850\u00a0km","21,196\u00a0km"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many kilometers of great wall of china?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["8,850\u00a0km","21,196\u00a0km"]},"style":"rule"},"extra_info":{"index":29,"split":"test"}}
{"id":"test_1718","question":"how many rooms is there in buckingham palace?","golden_answers":["775","775 rooms"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many rooms is there in buckingham palace?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["775","775 rooms"]},"style":"rule"},"extra_info":{"index":30,"split":"test"}}
{"id":"test_902","question":"who is the character of santa claus based on?","golden_answers":["Saint Nicholas","Father Christmas","Sinterklaas","Wodan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the character of santa claus based on?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Saint Nicholas","Father Christmas","Sinterklaas","Wodan"]},"style":"rule"},"extra_info":{"index":31,"split":"test"}}
{"id":"test_1839","question":"when did the sat become out of 1600?","golden_answers":["March 2016","2014","2016"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the sat become out of 1600?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["March 2016","2014","2016"]},"style":"rule"},"extra_info":{"index":32,"split":"test"}}
{"id":"test_2413","question":"when did the royal mint move to wales?","golden_answers":["1968","the 1960s.","17 December 1968","the 1960s"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the royal mint move to wales?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1968","the 1960s.","17 December 1968","the 1960s"]},"style":"rule"},"extra_info":{"index":33,"split":"test"}}
{"id":"test_1139","question":"what is the purpose of the bromophenol blue dye in the samples?","golden_answers":["as a pH indicator","a dye","a color marker","a pH indicator"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the purpose of the bromophenol blue dye in the samples?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["as a pH indicator","a dye","a color marker","a pH indicator"]},"style":"rule"},"extra_info":{"index":34,"split":"test"}}
{"id":"test_3315","question":"who was the chicago bears quarterback last year?","golden_answers":["Matt Barkley","Jay Cutler (5)","Matt Barkley (6)","Brian Hoyer","Jay Cutler","Brian Hoyer (5)"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the chicago bears quarterback last year?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Matt Barkley","Jay Cutler (5)","Matt Barkley (6)","Brian Hoyer","Jay Cutler","Brian Hoyer (5)"]},"style":"rule"},"extra_info":{"index":35,"split":"test"}}
{"id":"test_3560","question":"when was the movie the wizard of oz made?","golden_answers":["1939","August\u00a025,\u00a01939"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the movie the wizard of oz made?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1939","August\u00a025,\u00a01939"]},"style":"rule"},"extra_info":{"index":36,"split":"test"}}
{"id":"test_26","question":"where do they grow hops in the us?","golden_answers":["Yakima (Washington)","Idaho","Washington","western Canyon County, Idaho","Willamette (Oregon)","Oregon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where do they grow hops in the us?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Yakima (Washington)","Idaho","Washington","western Canyon County, Idaho","Willamette (Oregon)","Oregon"]},"style":"rule"},"extra_info":{"index":37,"split":"test"}}
{"id":"test_3108","question":"when did las vegas become a gambling town?","golden_answers":["1931","In 1931"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did las vegas become a gambling town?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1931","In 1931"]},"style":"rule"},"extra_info":{"index":38,"split":"test"}}
{"id":"test_3300","question":"when did the black death end in england?","golden_answers":["December 1349"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the black death end in england?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["December 1349"]},"style":"rule"},"extra_info":{"index":39,"split":"test"}}
{"id":"test_653","question":"when was the first star wars film released?","golden_answers":["1977","May\u00a025,\u00a01977"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the first star wars film released?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1977","May\u00a025,\u00a01977"]},"style":"rule"},"extra_info":{"index":40,"split":"test"}}
{"id":"test_2859","question":"when does rick and morty play on tv?","golden_answers":["late-night"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does rick and morty play on tv?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["late-night"]},"style":"rule"},"extra_info":{"index":41,"split":"test"}}
{"id":"test_1731","question":"what breed of cat has spots and stripes?","golden_answers":["tabby"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what breed of cat has spots and stripes?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["tabby"]},"style":"rule"},"extra_info":{"index":42,"split":"test"}}
{"id":"test_1393","question":"who picks the players in the nfl draft?","golden_answers":["each team"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who picks the players in the nfl draft?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["each team"]},"style":"rule"},"extra_info":{"index":43,"split":"test"}}
{"id":"test_1138","question":"who won an oscar for schindler's list?","golden_answers":["Steven Zaillian","Michael Kahn","Steven Spielberg","Janusz Kami\u0144ski","John Williams","Ewa Braun","Gerald R. Molen","Branko Lustig","Allan Starski"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won an oscar for schindler's list?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Steven Zaillian","Michael Kahn","Steven Spielberg","Janusz Kami\u0144ski","John Williams","Ewa Braun","Gerald R. Molen","Branko Lustig","Allan Starski"]},"style":"rule"},"extra_info":{"index":44,"split":"test"}}
{"id":"test_636","question":"the radiographic term used to describe the dense bone of the socket and septal crest is?","golden_answers":["lamina dura","alveolar process","the lamina dura"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the radiographic term used to describe the dense bone of the socket and septal crest is?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["lamina dura","alveolar process","the lamina dura"]},"style":"rule"},"extra_info":{"index":45,"split":"test"}}
{"id":"test_881","question":"who did the united states win its independence from?","golden_answers":["the British Empire","Great Britain"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who did the united states win its independence from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the British Empire","Great Britain"]},"style":"rule"},"extra_info":{"index":46,"split":"test"}}
{"id":"test_3127","question":"where are the winter olympic games being held this year?","golden_answers":["Pyeongchang County, South Korea"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where are the winter olympic games being held this year?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pyeongchang County, South Korea"]},"style":"rule"},"extra_info":{"index":47,"split":"test"}}
{"id":"test_1378","question":"who sang first line of we are the world?","golden_answers":["Lionel Richie","Kenny Rogers","James Ingram","Billy Joel","Tina Turner","Stevie Wonder","Paul Simon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang first line of we are the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Lionel Richie","Kenny Rogers","James Ingram","Billy Joel","Tina Turner","Stevie Wonder","Paul Simon"]},"style":"rule"},"extra_info":{"index":48,"split":"test"}}
{"id":"test_418","question":"in the early 1800s california society was dominated by which group?","golden_answers":["Spanish"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: in the early 1800s california society was dominated by which group?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Spanish"]},"style":"rule"},"extra_info":{"index":49,"split":"test"}}
{"id":"test_379","question":"who plays whitey bulger's girlfriend in black mass?","golden_answers":["actress Dakota Johnson","Dakota Johnson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays whitey bulger's girlfriend in black mass?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["actress Dakota Johnson","Dakota Johnson"]},"style":"rule"},"extra_info":{"index":50,"split":"test"}}
{"id":"test_1556","question":"where is the extensor pollicis longus tendon located?","golden_answers":["dorsally on the forearm","located dorsally on the forearm"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the extensor pollicis longus tendon located?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["dorsally on the forearm","located dorsally on the forearm"]},"style":"rule"},"extra_info":{"index":51,"split":"test"}}
{"id":"test_396","question":"where do you get a cashiers check from?","golden_answers":["a bank","bank"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where do you get a cashiers check from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["a bank","bank"]},"style":"rule"},"extra_info":{"index":52,"split":"test"}}
{"id":"test_1470","question":"when is season 8 for game of thrones?","golden_answers":["2019","in 2019"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is season 8 for game of thrones?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2019","in 2019"]},"style":"rule"},"extra_info":{"index":53,"split":"test"}}
{"id":"test_3471","question":"who was the successful commanding general of the northern forces in the civil war?","golden_answers":["George B. McClellan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the successful commanding general of the northern forces in the civil war?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["George B. McClellan"]},"style":"rule"},"extra_info":{"index":54,"split":"test"}}
{"id":"test_1408","question":"where was the 2015 rugby union world cup held?","golden_answers":["England","Wales"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the 2015 rugby union world cup held?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["England","Wales"]},"style":"rule"},"extra_info":{"index":55,"split":"test"}}
{"id":"test_2472","question":"when did they start 3 pointers in basketball?","golden_answers":["1961","1945"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did they start 3 pointers in basketball?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1961","1945"]},"style":"rule"},"extra_info":{"index":56,"split":"test"}}
{"id":"test_1083","question":"the octet rule states that in chemical compounds atoms tend to have the electron configuration of a?","golden_answers":["noble gas"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the octet rule states that in chemical compounds atoms tend to have the electron configuration of a?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["noble gas"]},"style":"rule"},"extra_info":{"index":57,"split":"test"}}
{"id":"test_3305","question":"who grows the most coffee in the world?","golden_answers":["Brazil"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who grows the most coffee in the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Brazil"]},"style":"rule"},"extra_info":{"index":58,"split":"test"}}
{"id":"test_177","question":"who had the longest tenure as moderator on meet the press?","golden_answers":["Tim Russert"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who had the longest tenure as moderator on meet the press?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Tim Russert"]},"style":"rule"},"extra_info":{"index":59,"split":"test"}}
{"id":"test_2988","question":"what was the main characteristic of post-world war ii american society?","golden_answers":["strong economic growth"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the main characteristic of post-world war ii american society?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["strong economic growth"]},"style":"rule"},"extra_info":{"index":60,"split":"test"}}
{"id":"test_1881","question":"who is the president of the republic of zambia?","golden_answers":["Edgar Lungu"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the president of the republic of zambia?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Edgar Lungu"]},"style":"rule"},"extra_info":{"index":61,"split":"test"}}
{"id":"test_2196","question":"how many super bowl games has the patriots played in?","golden_answers":["10","ten","ten times"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many super bowl games has the patriots played in?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["10","ten","ten times"]},"style":"rule"},"extra_info":{"index":62,"split":"test"}}
{"id":"test_511","question":"who has the most international goals of all time?","golden_answers":["Ali Daei"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most international goals of all time?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ali Daei"]},"style":"rule"},"extra_info":{"index":63,"split":"test"}}
{"id":"test_1550","question":"what was the real name of saudi arabia?","golden_answers":["the Saudi Arab kingdom"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the real name of saudi arabia?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the Saudi Arab kingdom"]},"style":"rule"},"extra_info":{"index":64,"split":"test"}}
{"id":"test_322","question":"dendrites and cell bodies are components of what type of matter found in the brain?","golden_answers":["gray","Grey matter"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: dendrites and cell bodies are components of what type of matter found in the brain?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["gray","Grey matter"]},"style":"rule"},"extra_info":{"index":65,"split":"test"}}
{"id":"test_2261","question":"product-market fit means being in a good market with a product that can satisfy that market?","golden_answers":["Mark Andreessen"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: product-market fit means being in a good market with a product that can satisfy that market?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mark Andreessen"]},"style":"rule"},"extra_info":{"index":66,"split":"test"}}
{"id":"test_1200","question":"when does the new season of are you the one come on?","golden_answers":["2018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does the new season of are you the one come on?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2018"]},"style":"rule"},"extra_info":{"index":67,"split":"test"}}
{"id":"test_3397","question":"who was the aztec ruler when the spanish arrived?","golden_answers":["Moctezuma II","emperor Cuauhtemoc"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the aztec ruler when the spanish arrived?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Moctezuma II","emperor Cuauhtemoc"]},"style":"rule"},"extra_info":{"index":68,"split":"test"}}
{"id":"test_2574","question":"who shot first in the shot heard around the world?","golden_answers":["Americans acting under orders"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who shot first in the shot heard around the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Americans acting under orders"]},"style":"rule"},"extra_info":{"index":69,"split":"test"}}
{"id":"test_2533","question":"who is darrell brother in the walking dead?","golden_answers":["Merle Dixon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is darrell brother in the walking dead?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Merle Dixon"]},"style":"rule"},"extra_info":{"index":70,"split":"test"}}
{"id":"test_3529","question":"who wrote the declaration of man and citizen?","golden_answers":["General Lafayette","Honor\u00e9 Mirabeau","Thomas Jefferson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the declaration of man and citizen?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["General Lafayette","Honor\u00e9 Mirabeau","Thomas Jefferson"]},"style":"rule"},"extra_info":{"index":71,"split":"test"}}
{"id":"test_1481","question":"who has won the most olympic medals in curling?","golden_answers":["Torger Nerg\u00e5rd","Eva Lund","Anette Norberg","Kevin Martin","Cathrine Lindahl","Anna Le Moine","Mirjam Ott"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has won the most olympic medals in curling?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Torger Nerg\u00e5rd","Eva Lund","Anette Norberg","Kevin Martin","Cathrine Lindahl","Anna Le Moine","Mirjam Ott"]},"style":"rule"},"extra_info":{"index":72,"split":"test"}}
{"id":"test_2364","question":"the golden age of india took place during the rule of the?","golden_answers":["Chandragupta II","Samudragupta","Chandragupta I","Gupta Empire","Vishnu Gupta","the Guptas","Sri-Gupta"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the golden age of india took place during the rule of the?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Chandragupta II","Samudragupta","Chandragupta I","Gupta Empire","Vishnu Gupta","the Guptas","Sri-Gupta"]},"style":"rule"},"extra_info":{"index":73,"split":"test"}}
{"id":"test_787","question":"when did the east india company take control of india?","golden_answers":["in 1757","1757","1799","1612"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the east india company take control of india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in 1757","1757","1799","1612"]},"style":"rule"},"extra_info":{"index":74,"split":"test"}}
{"id":"test_2885","question":"time of the state of the nation address?","golden_answers":["after 9pm\u00a0ET"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: time of the state of the nation address?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["after 9pm\u00a0ET"]},"style":"rule"},"extra_info":{"index":75,"split":"test"}}
{"id":"test_284","question":"when did the first train run in england?","golden_answers":["1560s","As early as 1671","1830","1804"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the first train run in england?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1560s","As early as 1671","1830","1804"]},"style":"rule"},"extra_info":{"index":76,"split":"test"}}
{"id":"test_187","question":"when did god save the queen became the national anthem?","golden_answers":["in the 1780s and 1790s","After the Battle of Culloden"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did god save the queen became the national anthem?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in the 1780s and 1790s","After the Battle of Culloden"]},"style":"rule"},"extra_info":{"index":77,"split":"test"}}
{"id":"test_2708","question":"who plays lady talisa in game of thrones?","golden_answers":["Oona Castilla Chaplin","Oona Chaplin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays lady talisa in game of thrones?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Oona Castilla Chaplin","Oona Chaplin"]},"style":"rule"},"extra_info":{"index":78,"split":"test"}}
{"id":"test_933","question":"what is the width of a cricket bat?","golden_answers":["no more than 4.25 inches"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the width of a cricket bat?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["no more than 4.25 inches"]},"style":"rule"},"extra_info":{"index":79,"split":"test"}}
{"id":"test_3166","question":"who got the first arjun award in athletics?","golden_answers":["Gurbachan Singh Randhawa"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who got the first arjun award in athletics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gurbachan Singh Randhawa"]},"style":"rule"},"extra_info":{"index":80,"split":"test"}}
{"id":"test_1185","question":"the first line of http request message is called ____?","golden_answers":["A request line","A status line","the status line","status line"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the first line of http request message is called ____?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["A request line","A status line","the status line","status line"]},"style":"rule"},"extra_info":{"index":81,"split":"test"}}
{"id":"test_326","question":"what is an example of a government monopoly in the united states?","golden_answers":["West Virginia American Water","State Bar of Arizona"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is an example of a government monopoly in the united states?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["West Virginia American Water","State Bar of Arizona"]},"style":"rule"},"extra_info":{"index":82,"split":"test"}}
{"id":"test_3503","question":"where was the louisiana purchase signed in 1803?","golden_answers":["Paris"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the louisiana purchase signed in 1803?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Paris"]},"style":"rule"},"extra_info":{"index":83,"split":"test"}}
{"id":"test_953","question":"what is the term for circular movement around a central point?","golden_answers":["angular rotation","Coriolis effect"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the term for circular movement around a central point?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["angular rotation","Coriolis effect"]},"style":"rule"},"extra_info":{"index":84,"split":"test"}}
{"id":"test_413","question":"when did they stop putting lead in paint?","golden_answers":["1992"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did they stop putting lead in paint?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1992"]},"style":"rule"},"extra_info":{"index":85,"split":"test"}}
{"id":"test_3558","question":"who is directly elected according to the constitution?","golden_answers":["senators"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is directly elected according to the constitution?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["senators"]},"style":"rule"},"extra_info":{"index":86,"split":"test"}}
{"id":"test_3565","question":"is aluminium a ferrous or non ferrous metal?","golden_answers":["non-ferrous"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: is aluminium a ferrous or non ferrous metal?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["non-ferrous"]},"style":"rule"},"extra_info":{"index":87,"split":"test"}}
{"id":"test_1857","question":"which approach to psychology focuses on the body especially the brain and nervous system?","golden_answers":["neuropsychology","Neuropsychology"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which approach to psychology focuses on the body especially the brain and nervous system?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["neuropsychology","Neuropsychology"]},"style":"rule"},"extra_info":{"index":88,"split":"test"}}
{"id":"test_2603","question":"how many episodes of the white princess will there be?","golden_answers":["eight","8"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes of the white princess will there be?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["eight","8"]},"style":"rule"},"extra_info":{"index":89,"split":"test"}}
{"id":"test_3416","question":"where can you find convergent boundaries on earth?","golden_answers":["Mariana Trench"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where can you find convergent boundaries on earth?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mariana Trench"]},"style":"rule"},"extra_info":{"index":90,"split":"test"}}
{"id":"test_1494","question":"distance from one side of a bridge to the other crossword?","golden_answers":["span"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: distance from one side of a bridge to the other crossword?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["span"]},"style":"rule"},"extra_info":{"index":91,"split":"test"}}
{"id":"test_666","question":"what type of novel is goodbye mr chips?","golden_answers":["Psychological fiction","novella"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of novel is goodbye mr chips?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Psychological fiction","novella"]},"style":"rule"},"extra_info":{"index":92,"split":"test"}}
{"id":"test_1516","question":"who is the designer in devil wears prada?","golden_answers":["Valentino Garavani"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the designer in devil wears prada?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Valentino Garavani"]},"style":"rule"},"extra_info":{"index":93,"split":"test"}}
{"id":"test_1455","question":"when was the last time it snowed in england on christmas day?","golden_answers":["2009"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time it snowed in england on christmas day?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2009"]},"style":"rule"},"extra_info":{"index":94,"split":"test"}}
{"id":"test_858","question":"who issued gold coins for the first time in india?","golden_answers":["Gupta Empire"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who issued gold coins for the first time in india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gupta Empire"]},"style":"rule"},"extra_info":{"index":95,"split":"test"}}
{"id":"test_2745","question":"where does water come from in new york city?","golden_answers":["The Catskill Aqueduct","The Delaware Aqueduct","The New Croton Aqueduct"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does water come from in new york city?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["The Catskill Aqueduct","The Delaware Aqueduct","The New Croton Aqueduct"]},"style":"rule"},"extra_info":{"index":96,"split":"test"}}
{"id":"test_1093","question":"how do you spell padawan from star wars?","golden_answers":["Padawan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how do you spell padawan from star wars?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Padawan"]},"style":"rule"},"extra_info":{"index":97,"split":"test"}}
{"id":"test_2874","question":"who expanded the palace of versailles to its present size?","golden_answers":["Louis XIV","Louis XV"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who expanded the palace of versailles to its present size?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Louis XIV","Louis XV"]},"style":"rule"},"extra_info":{"index":98,"split":"test"}}
{"id":"test_2799","question":"who does the voice of mrs. wolowitz on the big bang theory?","golden_answers":["Carol Ann Susi"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who does the voice of mrs. wolowitz on the big bang theory?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Carol Ann Susi"]},"style":"rule"},"extra_info":{"index":99,"split":"test"}}
{"id":"test_2654","question":"when did brent barry won the dunk contest?","golden_answers":["1996"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did brent barry won the dunk contest?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1996"]},"style":"rule"},"extra_info":{"index":100,"split":"test"}}
{"id":"test_292","question":"who is the book of galatians written to?","golden_answers":["the churches of Galatia"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the book of galatians written to?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the churches of Galatia"]},"style":"rule"},"extra_info":{"index":101,"split":"test"}}
{"id":"test_2495","question":"the heart muscle is stimulated to contract by electrical impulses which are generated where?","golden_answers":["the sinoatrial node"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the heart muscle is stimulated to contract by electrical impulses which are generated where?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the sinoatrial node"]},"style":"rule"},"extra_info":{"index":102,"split":"test"}}
{"id":"test_2600","question":"who won the icc under 19 world cup 2018?","golden_answers":["West Indies","India"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the icc under 19 world cup 2018?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["West Indies","India"]},"style":"rule"},"extra_info":{"index":103,"split":"test"}}
{"id":"test_700","question":"what was the religion in persia before islam?","golden_answers":["the Zoroastrian religion","Zoroastrian"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the religion in persia before islam?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the Zoroastrian religion","Zoroastrian"]},"style":"rule"},"extra_info":{"index":104,"split":"test"}}
{"id":"test_2187","question":"who stepped out of the dithyrambic chorus to become the first actor?","golden_answers":["Thespis"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who stepped out of the dithyrambic chorus to become the first actor?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Thespis"]},"style":"rule"},"extra_info":{"index":105,"split":"test"}}
{"id":"test_2986","question":"what type of bridge is the charles bridge?","golden_answers":["a bow bridge","bow bridge","Stone"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of bridge is the charles bridge?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["a bow bridge","bow bridge","Stone"]},"style":"rule"},"extra_info":{"index":106,"split":"test"}}
{"id":"test_1002","question":"where does puerto rico's power come from?","golden_answers":["Puerto Rico Electric Power Authority"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does puerto rico's power come from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Puerto Rico Electric Power Authority"]},"style":"rule"},"extra_info":{"index":107,"split":"test"}}
{"id":"test_669","question":"what happened at the 1939 worlds fair in regards to television?","golden_answers":["Television demonstrations are held"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what happened at the 1939 worlds fair in regards to television?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Television demonstrations are held"]},"style":"rule"},"extra_info":{"index":108,"split":"test"}}
{"id":"test_1893","question":"who sings i want to rock and roll?","golden_answers":["Kiss"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings i want to rock and roll?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kiss"]},"style":"rule"},"extra_info":{"index":109,"split":"test"}}
{"id":"test_1554","question":"what episode does caroline come into the originals?","golden_answers":["Brave New World"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what episode does caroline come into the originals?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Brave New World"]},"style":"rule"},"extra_info":{"index":110,"split":"test"}}
{"id":"test_1105","question":"what is the meaning of the name habib?","golden_answers":["\"beloved\"","beloved"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the meaning of the name habib?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["\"beloved\"","beloved"]},"style":"rule"},"extra_info":{"index":111,"split":"test"}}
{"id":"test_2621","question":"when were 7 books removed from the bible?","golden_answers":["1546"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when were 7 books removed from the bible?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1546"]},"style":"rule"},"extra_info":{"index":112,"split":"test"}}
{"id":"test_2818","question":"who plays the voice of chucky in seed of chucky?","golden_answers":["Brad Dourif"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the voice of chucky in seed of chucky?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Brad Dourif"]},"style":"rule"},"extra_info":{"index":113,"split":"test"}}
{"id":"test_2281","question":"who wrote the music for christmas story live?","golden_answers":["Pasek and Paul","Pasek & Paul","Justin Paul","Benj Pasek"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the music for christmas story live?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pasek and Paul","Pasek & Paul","Justin Paul","Benj Pasek"]},"style":"rule"},"extra_info":{"index":114,"split":"test"}}
{"id":"test_899","question":"who played mr. willoughby in sense and sensibility?","golden_answers":["Matthew Gregory Wise"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played mr. willoughby in sense and sensibility?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Matthew Gregory Wise"]},"style":"rule"},"extra_info":{"index":115,"split":"test"}}
{"id":"test_2804","question":"how many points are scored for a touchdown in american football?","golden_answers":["six","six points"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many points are scored for a touchdown in american football?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["six","six points"]},"style":"rule"},"extra_info":{"index":116,"split":"test"}}
{"id":"test_1328","question":"what was the purpose of the bantu education act?","golden_answers":["enforcing racially separated educational facilities"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the purpose of the bantu education act?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["enforcing racially separated educational facilities"]},"style":"rule"},"extra_info":{"index":117,"split":"test"}}
{"id":"test_3452","question":"who is super bowl 2018 half time show?","golden_answers":["Justin Timberlake"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is super bowl 2018 half time show?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Justin Timberlake"]},"style":"rule"},"extra_info":{"index":118,"split":"test"}}
{"id":"test_3147","question":"who played the girl in my two dads?","golden_answers":["Staci Keanan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played the girl in my two dads?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Staci Keanan"]},"style":"rule"},"extra_info":{"index":119,"split":"test"}}
{"id":"test_3178","question":"who wrote the guitar solo in beat it?","golden_answers":["Eddie Van Halen"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the guitar solo in beat it?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Eddie Van Halen"]},"style":"rule"},"extra_info":{"index":120,"split":"test"}}
{"id":"test_229","question":"when is the next episode of flash airing?","golden_answers":["May\u00a08,\u00a02018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is the next episode of flash airing?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["May\u00a08,\u00a02018"]},"style":"rule"},"extra_info":{"index":121,"split":"test"}}
{"id":"test_938","question":"oklahoma's 10 geographic regions are defined by surface features called?","golden_answers":["ecological regions"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: oklahoma's 10 geographic regions are defined by surface features called?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["ecological regions"]},"style":"rule"},"extra_info":{"index":122,"split":"test"}}
{"id":"test_3366","question":"who sang the song one of these nights?","golden_answers":["American rock band Eagles","the American rock band Eagles","Eagles"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang the song one of these nights?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["American rock band Eagles","the American rock band Eagles","Eagles"]},"style":"rule"},"extra_info":{"index":123,"split":"test"}}
{"id":"test_131","question":"who played stumpy in the movie rio bravo?","golden_answers":["Walter Brennan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played stumpy in the movie rio bravo?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Walter Brennan"]},"style":"rule"},"extra_info":{"index":124,"split":"test"}}
{"id":"test_3297","question":"who plays the scary nun in the conjuring 2?","golden_answers":["Bonnie Aarons"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the scary nun in the conjuring 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Bonnie Aarons"]},"style":"rule"},"extra_info":{"index":125,"split":"test"}}
{"id":"test_1292","question":"what percent of the us population controls the wealth?","golden_answers":["1%"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what percent of the us population controls the wealth?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1%"]},"style":"rule"},"extra_info":{"index":126,"split":"test"}}
{"id":"test_1643","question":"who is the premier of northern cape 2018?","golden_answers":["Sylvia Lucas"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the premier of northern cape 2018?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sylvia Lucas"]},"style":"rule"},"extra_info":{"index":127,"split":"test"}}
{"id":"test_1096","question":"who played in the stanley cup finals last year?","golden_answers":["Nashville Predators","Pittsburgh Penguins"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played in the stanley cup finals last year?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Nashville Predators","Pittsburgh Penguins"]},"style":"rule"},"extra_info":{"index":128,"split":"test"}}
{"id":"test_271","question":"when was the last time arsenal win premier league?","golden_answers":["2003\u201304"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time arsenal win premier league?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2003\u201304"]},"style":"rule"},"extra_info":{"index":129,"split":"test"}}
{"id":"test_864","question":"who plays meredith quill in guardians of the galaxy 2?","golden_answers":["Laura Jane Haddock"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays meredith quill in guardians of the galaxy 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Laura Jane Haddock"]},"style":"rule"},"extra_info":{"index":130,"split":"test"}}
{"id":"test_2323","question":"what happens when an air mass is pushed up and over a mountain range?","golden_answers":["Orographic lift"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what happens when an air mass is pushed up and over a mountain range?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Orographic lift"]},"style":"rule"},"extra_info":{"index":131,"split":"test"}}
{"id":"test_2940","question":"who created separation of powers and checks and balances?","golden_answers":["Montesquieu in the Enlightenment"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who created separation of powers and checks and balances?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Montesquieu in the Enlightenment"]},"style":"rule"},"extra_info":{"index":132,"split":"test"}}
{"id":"test_1288","question":"vapor pressure of water at 100c in torr?","golden_answers":["759.9625"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: vapor pressure of water at 100c in torr?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["759.9625"]},"style":"rule"},"extra_info":{"index":133,"split":"test"}}
{"id":"test_870","question":"where was 2017 beauty and the beast filmed?","golden_answers":["Surrey, United Kingdom"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was 2017 beauty and the beast filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Surrey, United Kingdom"]},"style":"rule"},"extra_info":{"index":134,"split":"test"}}
{"id":"test_2684","question":"who played doctor smith in lost in space?","golden_answers":["Jonathan Harris"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played doctor smith in lost in space?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jonathan Harris"]},"style":"rule"},"extra_info":{"index":135,"split":"test"}}
{"id":"test_2044","question":"who played the music producer in pitch perfect 2?","golden_answers":["Keegan-Michael Key"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played the music producer in pitch perfect 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Keegan-Michael Key"]},"style":"rule"},"extra_info":{"index":136,"split":"test"}}
{"id":"test_1620","question":"when was the last year the eagles went to the superbowl?","golden_answers":["following the 2017 season","2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last year the eagles went to the superbowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["following the 2017 season","2017"]},"style":"rule"},"extra_info":{"index":137,"split":"test"}}
{"id":"test_2633","question":"where did remember the titans camp take place?","golden_answers":["Gettysburg College"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did remember the titans camp take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gettysburg College"]},"style":"rule"},"extra_info":{"index":138,"split":"test"}}
{"id":"test_1879","question":"mount everest is part of what mountain range?","golden_answers":["Himalayas"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: mount everest is part of what mountain range?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Himalayas"]},"style":"rule"},"extra_info":{"index":139,"split":"test"}}
{"id":"test_585","question":"who are nominated for president of india 2017?","golden_answers":["Meira Kumar","Ram Nath Kovind"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who are nominated for president of india 2017?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Meira Kumar","Ram Nath Kovind"]},"style":"rule"},"extra_info":{"index":140,"split":"test"}}
{"id":"test_1084","question":"where is the oldest house in america located?","golden_answers":["Taos Pueblo"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the oldest house in america located?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Taos Pueblo"]},"style":"rule"},"extra_info":{"index":141,"split":"test"}}
{"id":"test_3602","question":"who were the two mathematicians that invented calculus?","golden_answers":["Gottfried Leibniz","Isaac Newton"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who were the two mathematicians that invented calculus?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gottfried Leibniz","Isaac Newton"]},"style":"rule"},"extra_info":{"index":142,"split":"test"}}
{"id":"test_1010","question":"what nba player has scored the most 3 pointers?","golden_answers":["Ray Allen"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what nba player has scored the most 3 pointers?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ray Allen"]},"style":"rule"},"extra_info":{"index":143,"split":"test"}}
{"id":"test_3051","question":"how many us states currently use capital punishment?","golden_answers":["31 states","31"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many us states currently use capital punishment?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["31 states","31"]},"style":"rule"},"extra_info":{"index":144,"split":"test"}}
{"id":"test_2299","question":"when does star wars battlefront 2 com out?","golden_answers":["November 17, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does star wars battlefront 2 com out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["November 17, 2017"]},"style":"rule"},"extra_info":{"index":145,"split":"test"}}
{"id":"test_2207","question":"when was the chain first used for f1?","golden_answers":["1978"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the chain first used for f1?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1978"]},"style":"rule"},"extra_info":{"index":146,"split":"test"}}
{"id":"test_1076","question":"who's the original singer of help me make it through the night?","golden_answers":["Kris Kristofferson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who's the original singer of help me make it through the night?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kris Kristofferson"]},"style":"rule"},"extra_info":{"index":147,"split":"test"}}
{"id":"test_3059","question":"who sang how long has this been going on song?","golden_answers":["Ace"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang how long has this been going on song?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ace"]},"style":"rule"},"extra_info":{"index":148,"split":"test"}}
{"id":"test_2394","question":"who played skeletor in the movie masters of the universe?","golden_answers":["Frank Langella"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played skeletor in the movie masters of the universe?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Frank Langella"]},"style":"rule"},"extra_info":{"index":149,"split":"test"}}
{"id":"test_1754","question":"who monitor the recovery of the location during a disaster?","golden_answers":["management team"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who monitor the recovery of the location during a disaster?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["management team"]},"style":"rule"},"extra_info":{"index":150,"split":"test"}}
{"id":"test_2390","question":"who wrote the song stop the world and let me off?","golden_answers":["W. S. Stevenson","Carl Belew"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the song stop the world and let me off?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["W. S. Stevenson","Carl Belew"]},"style":"rule"},"extra_info":{"index":151,"split":"test"}}
{"id":"test_1635","question":"who won battle of the sexes tennis game?","golden_answers":["Billie Jean King"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won battle of the sexes tennis game?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Billie Jean King"]},"style":"rule"},"extra_info":{"index":152,"split":"test"}}
{"id":"test_1482","question":"who played david on the assassination of gianni versace?","golden_answers":["Cody Fern"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played david on the assassination of gianni versace?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Cody Fern"]},"style":"rule"},"extra_info":{"index":153,"split":"test"}}
{"id":"test_898","question":"who won the men's single title of australia open on 1 february 2015?","golden_answers":["Djokovic","Novak Djokovic"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the men's single title of australia open on 1 february 2015?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Djokovic","Novak Djokovic"]},"style":"rule"},"extra_info":{"index":154,"split":"test"}}
{"id":"test_566","question":"who won the battle of saratoga in 1777?","golden_answers":["Americans","the Americans"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the battle of saratoga in 1777?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Americans","the Americans"]},"style":"rule"},"extra_info":{"index":155,"split":"test"}}
{"id":"test_2087","question":"where is the university of wisconsin madison located?","golden_answers":["Madison, Wisconsin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the university of wisconsin madison located?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Madison, Wisconsin"]},"style":"rule"},"extra_info":{"index":156,"split":"test"}}
{"id":"test_2021","question":"who came out first batman or spider man?","golden_answers":["Batman","Superman"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who came out first batman or spider man?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Batman","Superman"]},"style":"rule"},"extra_info":{"index":157,"split":"test"}}
{"id":"test_372","question":"where was the last scene of the danish girl filmed?","golden_answers":["the Mount Mannen in Norway"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the last scene of the danish girl filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the Mount Mannen in Norway"]},"style":"rule"},"extra_info":{"index":158,"split":"test"}}
{"id":"test_3095","question":"who does the voice of the gorilla in the movie sing?","golden_answers":["Taron Egerton"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who does the voice of the gorilla in the movie sing?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Taron Egerton"]},"style":"rule"},"extra_info":{"index":159,"split":"test"}}
{"id":"test_192","question":"who lives in the blue house in balamory?","golden_answers":["Edie McCredie"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who lives in the blue house in balamory?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Edie McCredie"]},"style":"rule"},"extra_info":{"index":160,"split":"test"}}
{"id":"test_449","question":"where did they film the show the crossing?","golden_answers":["British Columbia, Canada"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did they film the show the crossing?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["British Columbia, Canada"]},"style":"rule"},"extra_info":{"index":161,"split":"test"}}
{"id":"test_626","question":"when did britain set up east indian trading company?","golden_answers":["31 December 1600"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did britain set up east indian trading company?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["31 December 1600"]},"style":"rule"},"extra_info":{"index":162,"split":"test"}}
{"id":"test_2570","question":"who sold the most records elvis or the beatles?","golden_answers":["The Beatles"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sold the most records elvis or the beatles?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["The Beatles"]},"style":"rule"},"extra_info":{"index":163,"split":"test"}}
{"id":"test_655","question":"epidemiologists attempt to explain the link between health and variables such as?","golden_answers":["biological agents","disease conditions in defined populations","smoking","stress","chemicals","alcohol"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: epidemiologists attempt to explain the link between health and variables such as?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["biological agents","disease conditions in defined populations","smoking","stress","chemicals","alcohol"]},"style":"rule"},"extra_info":{"index":164,"split":"test"}}
{"id":"test_3244","question":"when is the flash coming back after christmas?","golden_answers":["January\u00a016,\u00a02018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is the flash coming back after christmas?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["January\u00a016,\u00a02018"]},"style":"rule"},"extra_info":{"index":165,"split":"test"}}
{"id":"test_2787","question":"who sang the song tell me something good?","golden_answers":["Rufus and Chaka Khan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang the song tell me something good?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rufus and Chaka Khan"]},"style":"rule"},"extra_info":{"index":166,"split":"test"}}
{"id":"test_1729","question":"what is a coherent set of values and beliefs about public policy called?","golden_answers":["a political ideology","political ideology","ideology"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is a coherent set of values and beliefs about public policy called?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["a political ideology","political ideology","ideology"]},"style":"rule"},"extra_info":{"index":167,"split":"test"}}
{"id":"test_2442","question":"what river is associated with the city of rome?","golden_answers":["The Tiber","Tiber"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what river is associated with the city of rome?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["The Tiber","Tiber"]},"style":"rule"},"extra_info":{"index":168,"split":"test"}}
{"id":"test_260","question":"who played susanna in legends of the fall?","golden_answers":["Julia Ormond"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played susanna in legends of the fall?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Julia Ormond"]},"style":"rule"},"extra_info":{"index":169,"split":"test"}}
{"id":"test_1576","question":"is parallax more pronounced with nearby stars or with distant stars?","golden_answers":["nearby objects","nearby"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: is parallax more pronounced with nearby stars or with distant stars?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["nearby objects","nearby"]},"style":"rule"},"extra_info":{"index":170,"split":"test"}}
{"id":"test_1563","question":"where does saying bob's your uncle come from?","golden_answers":["unknown origin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does saying bob's your uncle come from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["unknown origin"]},"style":"rule"},"extra_info":{"index":171,"split":"test"}}
{"id":"test_2440","question":"when was the last time michigan won the championship?","golden_answers":["1989"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time michigan won the championship?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1989"]},"style":"rule"},"extra_info":{"index":172,"split":"test"}}
{"id":"test_1917","question":"what is the current rate of interest on ppf?","golden_answers":["7.6% Per Annum","7.6%"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the current rate of interest on ppf?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["7.6% Per Annum","7.6%"]},"style":"rule"},"extra_info":{"index":173,"split":"test"}}
{"id":"test_2167","question":"when were the winnie the pooh books written?","golden_answers":["1924","1926","1927","1928"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when were the winnie the pooh books written?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1924","1926","1927","1928"]},"style":"rule"},"extra_info":{"index":174,"split":"test"}}
{"id":"test_1029","question":"which level of weight bearing often comes with a set number of pounds?","golden_answers":["Partial weight-bearing"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which level of weight bearing often comes with a set number of pounds?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Partial weight-bearing"]},"style":"rule"},"extra_info":{"index":175,"split":"test"}}
{"id":"test_2266","question":"where does sex and the city take place?","golden_answers":["New York City"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does sex and the city take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["New York City"]},"style":"rule"},"extra_info":{"index":176,"split":"test"}}
{"id":"test_47","question":"what was the city of beijing previously known as?","golden_answers":["Peking"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the city of beijing previously known as?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Peking"]},"style":"rule"},"extra_info":{"index":177,"split":"test"}}
{"id":"test_2786","question":"where does aarp fall on the political spectrum?","golden_answers":["non-partisan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does aarp fall on the political spectrum?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["non-partisan"]},"style":"rule"},"extra_info":{"index":178,"split":"test"}}
{"id":"test_2952","question":"which greek god flew too close to the sun?","golden_answers":["Icarus"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which greek god flew too close to the sun?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Icarus"]},"style":"rule"},"extra_info":{"index":179,"split":"test"}}
{"id":"test_469","question":"when was the last time stock market crashed?","golden_answers":["27 Oct 1997","18 August 2015"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time stock market crashed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["27 Oct 1997","18 August 2015"]},"style":"rule"},"extra_info":{"index":180,"split":"test"}}
{"id":"test_2792","question":"where do they film young and the restless?","golden_answers":["CBS Television City"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where do they film young and the restless?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["CBS Television City"]},"style":"rule"},"extra_info":{"index":181,"split":"test"}}
{"id":"test_2199","question":"where is the setting for beauty and the beast?","golden_answers":["Rococo-era France","France"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the setting for beauty and the beast?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rococo-era France","France"]},"style":"rule"},"extra_info":{"index":182,"split":"test"}}
{"id":"test_3075","question":"who was selected for the 2018 football hall of fame?","golden_answers":["Ray Lewis","Brian Urlacher","Jerry Kramer","Robert Brazile","Bobby Beathard","Brian Dawkins","Randy Moss","Terrell Owens"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was selected for the 2018 football hall of fame?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ray Lewis","Brian Urlacher","Jerry Kramer","Robert Brazile","Bobby Beathard","Brian Dawkins","Randy Moss","Terrell Owens"]},"style":"rule"},"extra_info":{"index":183,"split":"test"}}
{"id":"test_1092","question":"who appoints the chair of the federal reserve system?","golden_answers":["President of the United States"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who appoints the chair of the federal reserve system?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["President of the United States"]},"style":"rule"},"extra_info":{"index":184,"split":"test"}}
{"id":"test_3148","question":"what type of planet is neptune known as?","golden_answers":["giant","ice giants"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of planet is neptune known as?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["giant","ice giants"]},"style":"rule"},"extra_info":{"index":185,"split":"test"}}
{"id":"test_2625","question":"who has the most super bowls in nfl history?","golden_answers":["Pittsburgh Steelers","The Pittsburgh Steelers"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most super bowls in nfl history?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pittsburgh Steelers","The Pittsburgh Steelers"]},"style":"rule"},"extra_info":{"index":186,"split":"test"}}
{"id":"test_3566","question":"who won the king of dance season 2?","golden_answers":["LAAB Crew From Team Sherif"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the king of dance season 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["LAAB Crew From Team Sherif"]},"style":"rule"},"extra_info":{"index":187,"split":"test"}}
{"id":"test_3608","question":"when was as you like it first performed?","golden_answers":["1603"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was as you like it first performed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1603"]},"style":"rule"},"extra_info":{"index":188,"split":"test"}}
{"id":"test_1202","question":"what is the meaning of cc and bcc?","golden_answers":["Carbon copy","Carbon copy to secondary recipients","Blind carbon copy"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the meaning of cc and bcc?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Carbon copy","Carbon copy to secondary recipients","Blind carbon copy"]},"style":"rule"},"extra_info":{"index":189,"split":"test"}}
{"id":"test_1780","question":"list of strict nature reserve in the philippines?","golden_answers":["Palawan","Lake Malimanga","Olango Island","Calauit Safari Park","Lake Buluan","Calavite and FB Harrison"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: list of strict nature reserve in the philippines?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Palawan","Lake Malimanga","Olango Island","Calauit Safari Park","Lake Buluan","Calavite and FB Harrison"]},"style":"rule"},"extra_info":{"index":190,"split":"test"}}
{"id":"test_647","question":"when did the us stop trading with japan?","golden_answers":["1939","1940"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the us stop trading with japan?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1939","1940"]},"style":"rule"},"extra_info":{"index":191,"split":"test"}}
{"id":"test_1858","question":"who sings the song i want to go outside in the rain?","golden_answers":["Milira"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the song i want to go outside in the rain?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Milira"]},"style":"rule"},"extra_info":{"index":192,"split":"test"}}
{"id":"test_13","question":"cast of law & order special victim unit?","golden_answers":["Kelli Giddish","Richard Belzer","Stephanie March","Diane Neal","Ice-T","Danny Pino","Dann Florek","Tamara Tunie","Michaela McManus","Mariska Hargitay","Adam Beach","B. D. Wong","Christopher Meloni","Ra\u00fal Esparza","Michelle Hurd","Peter Scanavino"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: cast of law & order special victim unit?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kelli Giddish","Richard Belzer","Stephanie March","Diane Neal","Ice-T","Danny Pino","Dann Florek","Tamara Tunie","Michaela McManus","Mariska Hargitay","Adam Beach","B. D. Wong","Christopher Meloni","Ra\u00fal Esparza","Michelle Hurd","Peter Scanavino"]},"style":"rule"},"extra_info":{"index":193,"split":"test"}}
{"id":"test_2957","question":"how much money did the film titanic make?","golden_answers":["$2.18 billion","$2.187 billion"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how much money did the film titanic make?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["$2.18 billion","$2.187 billion"]},"style":"rule"},"extra_info":{"index":194,"split":"test"}}
{"id":"test_2947","question":"what is the pirates of the caribbean in order?","golden_answers":["On Stranger Tides","At World's End","Dead Men Tell No Tales","Dead Man's Chest"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the pirates of the caribbean in order?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["On Stranger Tides","At World's End","Dead Men Tell No Tales","Dead Man's Chest"]},"style":"rule"},"extra_info":{"index":195,"split":"test"}}
{"id":"test_1078","question":"what is the big gold dome in jerusalem?","golden_answers":["Dome of the Rock"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the big gold dome in jerusalem?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Dome of the Rock"]},"style":"rule"},"extra_info":{"index":196,"split":"test"}}
{"id":"test_2050","question":"what is the purse for the senior open?","golden_answers":["$2 million in 2011"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the purse for the senior open?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["$2 million in 2011"]},"style":"rule"},"extra_info":{"index":197,"split":"test"}}
{"id":"test_3120","question":"who plays brad pitt's daughter in moneyball?","golden_answers":["Kerris Lilla Dorsey"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays brad pitt's daughter in moneyball?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kerris Lilla Dorsey"]},"style":"rule"},"extra_info":{"index":198,"split":"test"}}
{"id":"test_731","question":"who was the pinkerton detective agency's first female detective?","golden_answers":["Kate Warne"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the pinkerton detective agency's first female detective?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kate Warne"]},"style":"rule"},"extra_info":{"index":199,"split":"test"}}
{"id":"test_2079","question":"who is the actor that plays dr. sean murphy?","golden_answers":["Freddie Highmore"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the actor that plays dr. sean murphy?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Freddie Highmore"]},"style":"rule"},"extra_info":{"index":200,"split":"test"}}
{"id":"test_435","question":"who wrote the original little red riding hood story?","golden_answers":["Charles Perrault"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the original little red riding hood story?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Charles Perrault"]},"style":"rule"},"extra_info":{"index":201,"split":"test"}}
{"id":"test_2561","question":"who sings the songs in crazy ex girlfriend?","golden_answers":["Rachel Bloom"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the songs in crazy ex girlfriend?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rachel Bloom"]},"style":"rule"},"extra_info":{"index":202,"split":"test"}}
{"id":"test_1222","question":"what class of ship is the carnival glory?","golden_answers":["Conquest","Conquest-class cruise ship"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what class of ship is the carnival glory?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Conquest","Conquest-class cruise ship"]},"style":"rule"},"extra_info":{"index":203,"split":"test"}}
{"id":"test_2617","question":"unsaturated fats are comprised of lipids that contain?","golden_answers":["double bond","at least one double bond"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: unsaturated fats are comprised of lipids that contain?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["double bond","at least one double bond"]},"style":"rule"},"extra_info":{"index":204,"split":"test"}}
{"id":"test_3409","question":"what is the definition of the word hosanna?","golden_answers":["save, rescue, savior","rescue","savior","save"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the definition of the word hosanna?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["save, rescue, savior","rescue","savior","save"]},"style":"rule"},"extra_info":{"index":205,"split":"test"}}
{"id":"test_2494","question":"when did macbook pro 13 inch come out?","golden_answers":["October 2008","June 8, 2009"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did macbook pro 13 inch come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["October 2008","June 8, 2009"]},"style":"rule"},"extra_info":{"index":206,"split":"test"}}
{"id":"test_3584","question":"what is the setting of the story sorry wrong number?","golden_answers":["Manhattan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the setting of the story sorry wrong number?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Manhattan"]},"style":"rule"},"extra_info":{"index":207,"split":"test"}}
{"id":"test_3447","question":"what does aa on a license plate mean?","golden_answers":["cars of the royal family"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what does aa on a license plate mean?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["cars of the royal family"]},"style":"rule"},"extra_info":{"index":208,"split":"test"}}
{"id":"test_1531","question":"what are the active materials of a lead acid battery?","golden_answers":["Lead","sulfuric acid","Lead and lead dioxide","lead dioxide"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what are the active materials of a lead acid battery?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Lead","sulfuric acid","Lead and lead dioxide","lead dioxide"]},"style":"rule"},"extra_info":{"index":209,"split":"test"}}
{"id":"test_3123","question":"which nfl coach has the most superbowl rings?","golden_answers":["Bill Belichick"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which nfl coach has the most superbowl rings?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Bill Belichick"]},"style":"rule"},"extra_info":{"index":210,"split":"test"}}
{"id":"test_661","question":"what written material is included in the talmud?","golden_answers":["the Mishnah","the Gemara"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what written material is included in the talmud?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the Mishnah","the Gemara"]},"style":"rule"},"extra_info":{"index":211,"split":"test"}}
{"id":"test_2209","question":"what year is the deer hunter set in?","golden_answers":["late 1967","1967"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what year is the deer hunter set in?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["late 1967","1967"]},"style":"rule"},"extra_info":{"index":212,"split":"test"}}
{"id":"test_3189","question":"who sings gone gone gone she been gone so long?","golden_answers":["Chilliwack"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings gone gone gone she been gone so long?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Chilliwack"]},"style":"rule"},"extra_info":{"index":213,"split":"test"}}
{"id":"test_2172","question":"wolf of wall street number of f words?","golden_answers":["569"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: wolf of wall street number of f words?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["569"]},"style":"rule"},"extra_info":{"index":214,"split":"test"}}
{"id":"test_2","question":"which mode is used for short wave broadcast service?","golden_answers":["Olivia","MFSK"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which mode is used for short wave broadcast service?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Olivia","MFSK"]},"style":"rule"},"extra_info":{"index":215,"split":"test"}}
{"id":"test_2453","question":"what are the colors of the netherlands flag?","golden_answers":["blue","white","red"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what are the colors of the netherlands flag?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["blue","white","red"]},"style":"rule"},"extra_info":{"index":216,"split":"test"}}
{"id":"test_1327","question":"who does the democratic republic of congo trade with?","golden_answers":["Group of 77","WTO","Zimbabwe","Belgium","SADC","Kenya","South Africa","China","AU","Zambia","IMF","World Bank","African Development Bank","France"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who does the democratic republic of congo trade with?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Group of 77","WTO","Zimbabwe","Belgium","SADC","Kenya","South Africa","China","AU","Zambia","IMF","World Bank","African Development Bank","France"]},"style":"rule"},"extra_info":{"index":217,"split":"test"}}
{"id":"test_2001","question":"where does the sciatic nerve run in the foot?","golden_answers":["on the posterior aspect"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the sciatic nerve run in the foot?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["on the posterior aspect"]},"style":"rule"},"extra_info":{"index":218,"split":"test"}}
{"id":"test_79","question":"when do primary ossification centers appear in an embryo?","golden_answers":["prenatal development"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when do primary ossification centers appear in an embryo?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["prenatal development"]},"style":"rule"},"extra_info":{"index":219,"split":"test"}}
{"id":"test_458","question":"who wrote put your hand in the hand of the man who stilled the water?","golden_answers":["Gene MacLellan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote put your hand in the hand of the man who stilled the water?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gene MacLellan"]},"style":"rule"},"extra_info":{"index":220,"split":"test"}}
{"id":"test_1486","question":"where does the path train stop in newark?","golden_answers":["Newark Penn Station"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the path train stop in newark?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Newark Penn Station"]},"style":"rule"},"extra_info":{"index":221,"split":"test"}}
{"id":"test_3304","question":"where was the movie the glass castle filmed?","golden_answers":["in Welch, West Virginia","Welch, West Virginia"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the movie the glass castle filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in Welch, West Virginia","Welch, West Virginia"]},"style":"rule"},"extra_info":{"index":222,"split":"test"}}
{"id":"test_1259","question":"who built pedestrian bridge at florida international university?","golden_answers":["Munilla Construction Management","FIGG Bridge Engineers"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who built pedestrian bridge at florida international university?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Munilla Construction Management","FIGG Bridge Engineers"]},"style":"rule"},"extra_info":{"index":223,"split":"test"}}
{"id":"test_980","question":"where did the rulers of the qing dynasty originate?","golden_answers":["Manchuria"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did the rulers of the qing dynasty originate?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Manchuria"]},"style":"rule"},"extra_info":{"index":224,"split":"test"}}
{"id":"test_237","question":"who sings why does it hurt when i pee?","golden_answers":["Frank Zappa","Frank Zappa's"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings why does it hurt when i pee?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Frank Zappa","Frank Zappa's"]},"style":"rule"},"extra_info":{"index":225,"split":"test"}}
{"id":"test_986","question":"who was the top scorer in 2014 world cup?","golden_answers":["James Rodr\u00edguez"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the top scorer in 2014 world cup?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["James Rodr\u00edguez"]},"style":"rule"},"extra_info":{"index":226,"split":"test"}}
{"id":"test_3478","question":"who won the academy award for the deer hunter?","golden_answers":["John Peverall","Michael Deeley","Peter Zinner","William L. McCaughey","Michael Cimino","Barry Spikings","Richard Portman","Aaron Rochin","C. Darin Knight","Christopher Walken"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the academy award for the deer hunter?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["John Peverall","Michael Deeley","Peter Zinner","William L. McCaughey","Michael Cimino","Barry Spikings","Richard Portman","Aaron Rochin","C. Darin Knight","Christopher Walken"]},"style":"rule"},"extra_info":{"index":227,"split":"test"}}
{"id":"test_3544","question":"when does the new mlp movie come out?","golden_answers":["September 24, 2017","October\u00a06,\u00a02017","October 6, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does the new mlp movie come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["September 24, 2017","October\u00a06,\u00a02017","October 6, 2017"]},"style":"rule"},"extra_info":{"index":228,"split":"test"}}
{"id":"test_350","question":"who plays jack skellington in nightmare before christmas?","golden_answers":["Danny Elfman","Chris Sarandon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays jack skellington in nightmare before christmas?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Danny Elfman","Chris Sarandon"]},"style":"rule"},"extra_info":{"index":229,"split":"test"}}
{"id":"test_2997","question":"who took the first steps on the moon in 1969?","golden_answers":["Neil Armstrong"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who took the first steps on the moon in 1969?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Neil Armstrong"]},"style":"rule"},"extra_info":{"index":230,"split":"test"}}
{"id":"test_1990","question":"where was the tv show in the heat of the night filmed?","golden_answers":["Decatur in Dekalb County","Atlanta","Covington, Georgia","Hammond, Louisiana"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the tv show in the heat of the night filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Decatur in Dekalb County","Atlanta","Covington, Georgia","Hammond, Louisiana"]},"style":"rule"},"extra_info":{"index":231,"split":"test"}}
{"id":"test_3342","question":"who won the most stanley cups in history?","golden_answers":["Montreal Canadiens"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the most stanley cups in history?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Montreal Canadiens"]},"style":"rule"},"extra_info":{"index":232,"split":"test"}}
{"id":"test_283","question":"when was son of a preacher man released?","golden_answers":["late 1968","November 8, 1968"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was son of a preacher man released?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["late 1968","November 8, 1968"]},"style":"rule"},"extra_info":{"index":233,"split":"test"}}
{"id":"test_3115","question":"when did clifford the big red dog first air on tv?","golden_answers":["September 4, 2000"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did clifford the big red dog first air on tv?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["September 4, 2000"]},"style":"rule"},"extra_info":{"index":234,"split":"test"}}
{"id":"test_2181","question":"which country has won maximum number of gold medal in asian game 2014?","golden_answers":["China"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which country has won maximum number of gold medal in asian game 2014?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["China"]},"style":"rule"},"extra_info":{"index":235,"split":"test"}}
{"id":"test_3136","question":"who can be called a man of god?","golden_answers":["beloved religious leaders","prophets"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who can be called a man of god?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["beloved religious leaders","prophets"]},"style":"rule"},"extra_info":{"index":236,"split":"test"}}
{"id":"test_515","question":"what is the word for clarified butter in the balkans and middle east?","golden_answers":["smen"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the word for clarified butter in the balkans and middle east?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["smen"]},"style":"rule"},"extra_info":{"index":237,"split":"test"}}
{"id":"test_525","question":"who was the qb for the saints before drew brees?","golden_answers":["Aaron Brooks"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the qb for the saints before drew brees?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Aaron Brooks"]},"style":"rule"},"extra_info":{"index":238,"split":"test"}}
{"id":"test_2702","question":"when did rachel have her baby on friends?","golden_answers":["May\u00a016,\u00a02002"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did rachel have her baby on friends?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["May\u00a016,\u00a02002"]},"style":"rule"},"extra_info":{"index":239,"split":"test"}}
{"id":"test_1946","question":"what was the actual year that the movie regarding the titans took place?","golden_answers":["1971"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the actual year that the movie regarding the titans took place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1971"]},"style":"rule"},"extra_info":{"index":240,"split":"test"}}
{"id":"test_2251","question":"what is the name of season 6 of american horror story?","golden_answers":["Roanoke","American Horror Story: Roanoke"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the name of season 6 of american horror story?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Roanoke","American Horror Story: Roanoke"]},"style":"rule"},"extra_info":{"index":241,"split":"test"}}
{"id":"test_676","question":"which condition would most likely require nutrition delivered through tpn?","golden_answers":["bowel obstruction","ulcerative colitis","high-output fistula","short bowel syndrome","very severe Crohn's disease","prolonged diarrhea","gastroschisis"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which condition would most likely require nutrition delivered through tpn?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["bowel obstruction","ulcerative colitis","high-output fistula","short bowel syndrome","very severe Crohn's disease","prolonged diarrhea","gastroschisis"]},"style":"rule"},"extra_info":{"index":242,"split":"test"}}
{"id":"test_1085","question":"who was the walker rick killed in the first episode?","golden_answers":["Addy Miller"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the walker rick killed in the first episode?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Addy Miller"]},"style":"rule"},"extra_info":{"index":243,"split":"test"}}
{"id":"test_2161","question":"where did aeneas go when he left carthage?","golden_answers":["Sicily"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did aeneas go when he left carthage?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sicily"]},"style":"rule"},"extra_info":{"index":244,"split":"test"}}
{"id":"test_2484","question":"who won the award for best goalkeeper in football world cup 2006?","golden_answers":["Gianluigi Buffon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the award for best goalkeeper in football world cup 2006?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gianluigi Buffon"]},"style":"rule"},"extra_info":{"index":245,"split":"test"}}
{"id":"test_1733","question":"when did the angel of the north get built?","golden_answers":["1998","1994"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the angel of the north get built?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1998","1994"]},"style":"rule"},"extra_info":{"index":246,"split":"test"}}
{"id":"test_867","question":"locations for the film an englishman who went up a hill?","golden_answers":["Llanrhaeadr-ym-Mochnant","Llansilin in Powys"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: locations for the film an englishman who went up a hill?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Llanrhaeadr-ym-Mochnant","Llansilin in Powys"]},"style":"rule"},"extra_info":{"index":247,"split":"test"}}
{"id":"test_2208","question":"when was you'll never walk alone first released?","golden_answers":["1945"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was you'll never walk alone first released?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1945"]},"style":"rule"},"extra_info":{"index":248,"split":"test"}}
{"id":"test_3093","question":"where was the remake of dirty dancing filmed?","golden_answers":["High Hampton Inn in Cashiers","Hendersonville, North Carolina","western North Carolina"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the remake of dirty dancing filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["High Hampton Inn in Cashiers","Hendersonville, North Carolina","western North Carolina"]},"style":"rule"},"extra_info":{"index":249,"split":"test"}}
{"id":"test_2989","question":"which way does the earth orbit the sun?","golden_answers":["counterclockwise","counterclockwise direction"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which way does the earth orbit the sun?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["counterclockwise","counterclockwise direction"]},"style":"rule"},"extra_info":{"index":250,"split":"test"}}
{"id":"test_2825","question":"who is singing in something just like this?","golden_answers":["Will Champion","Coldplay","The Chainsmokers","Chris Martin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is singing in something just like this?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Will Champion","Coldplay","The Chainsmokers","Chris Martin"]},"style":"rule"},"extra_info":{"index":251,"split":"test"}}
{"id":"test_823","question":"where do the signals for apoptosis come from?","golden_answers":["from other cells"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where do the signals for apoptosis come from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["from other cells"]},"style":"rule"},"extra_info":{"index":252,"split":"test"}}
{"id":"test_2920","question":"who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia?","golden_answers":["Shahzar Rizvi"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Shahzar Rizvi"]},"style":"rule"},"extra_info":{"index":253,"split":"test"}}
{"id":"test_1276","question":"who wrote the phantom of the opera music?","golden_answers":["Andrew Lloyd Webber"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the phantom of the opera music?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Andrew Lloyd Webber"]},"style":"rule"},"extra_info":{"index":254,"split":"test"}}
{"id":"test_1634","question":"when did fortnite save the world first come out?","golden_answers":["July 25, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did fortnite save the world first come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["July 25, 2017"]},"style":"rule"},"extra_info":{"index":255,"split":"test"}}
{"id":"test_2751","question":"who win road march in trinidad and tobago?","golden_answers":["Superblue","Machel Montano"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who win road march in trinidad and tobago?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Superblue","Machel Montano"]},"style":"rule"},"extra_info":{"index":256,"split":"test"}}
{"id":"test_3582","question":"when does the movie the star come out?","golden_answers":["November 17, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does the movie the star come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["November 17, 2017"]},"style":"rule"},"extra_info":{"index":257,"split":"test"}}
{"id":"test_1529","question":"when did reba mcentire record back to god?","golden_answers":["February 3, 2017","2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did reba mcentire record back to god?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["February 3, 2017","2017"]},"style":"rule"},"extra_info":{"index":258,"split":"test"}}
{"id":"test_1794","question":"who did the singing in into the woods?","golden_answers":["the cast","the cast members"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who did the singing in into the woods?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the cast","the cast members"]},"style":"rule"},"extra_info":{"index":259,"split":"test"}}
{"id":"test_2119","question":"who plays max voice in a goofy movie?","golden_answers":["Jason Marsden"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays max voice in a goofy movie?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jason Marsden"]},"style":"rule"},"extra_info":{"index":260,"split":"test"}}
{"id":"test_1849","question":"what are the 5 prohibitions of yom kippur?","golden_answers":["No marital relations","No wearing of leather shoes","No eating and drinking","No bathing or washing"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what are the 5 prohibitions of yom kippur?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["No marital relations","No wearing of leather shoes","No eating and drinking","No bathing or washing"]},"style":"rule"},"extra_info":{"index":261,"split":"test"}}
{"id":"test_495","question":"where did the book small steps take place?","golden_answers":["Austin, Texas"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did the book small steps take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Austin, Texas"]},"style":"rule"},"extra_info":{"index":262,"split":"test"}}
{"id":"test_1015","question":"what level is a city and guilds qualification?","golden_answers":["entry level to level 7"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what level is a city and guilds qualification?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["entry level to level 7"]},"style":"rule"},"extra_info":{"index":263,"split":"test"}}
{"id":"test_920","question":"where does the red wolf live in the world?","golden_answers":["the southeastern United States","southeastern United States"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the red wolf live in the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the southeastern United States","southeastern United States"]},"style":"rule"},"extra_info":{"index":264,"split":"test"}}
{"id":"test_262","question":"fast & furious 8 release date in india?","golden_answers":["April 12, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: fast & furious 8 release date in india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["April 12, 2017"]},"style":"rule"},"extra_info":{"index":265,"split":"test"}}
{"id":"test_1384","question":"where does the edinburgh fringe festival take place?","golden_answers":["in Edinburgh, Scotland"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the edinburgh fringe festival take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in Edinburgh, Scotland"]},"style":"rule"},"extra_info":{"index":266,"split":"test"}}
{"id":"test_86","question":"who wrote there's a guy works down the chip shop lyrics?","golden_answers":["Philip Rambow","Kirsty MacColl\/Philip Rambow","Kirsty MacColl"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote there's a guy works down the chip shop lyrics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Philip Rambow","Kirsty MacColl\/Philip Rambow","Kirsty MacColl"]},"style":"rule"},"extra_info":{"index":267,"split":"test"}}
{"id":"test_2409","question":"the creation of human beings in the kumulipo happens during which w\u0101 or period of creation?","golden_answers":["In the ninth w\u0101","the ninth w\u0101,","the ninth w\u0101","the ninth"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the creation of human beings in the kumulipo happens during which w\u0101 or period of creation?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["In the ninth w\u0101","the ninth w\u0101,","the ninth w\u0101","the ninth"]},"style":"rule"},"extra_info":{"index":268,"split":"test"}}
{"id":"test_2268","question":"what year did the golden state warriors win their first nba championship?","golden_answers":["1947"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what year did the golden state warriors win their first nba championship?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1947"]},"style":"rule"},"extra_info":{"index":269,"split":"test"}}
{"id":"test_942","question":"most passing yards in nfl history in a game?","golden_answers":["Norm Van Brocklin","554"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: most passing yards in nfl history in a game?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Norm Van Brocklin","554"]},"style":"rule"},"extra_info":{"index":270,"split":"test"}}
{"id":"test_2410","question":"when did the democratic party change its name?","golden_answers":["the 1830s"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the democratic party change its name?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the 1830s"]},"style":"rule"},"extra_info":{"index":271,"split":"test"}}
{"id":"test_3578","question":"who is the singer of kal ho na ho?","golden_answers":["Sonu Nigam","Richa Sharma","Alka Yagnik"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the singer of kal ho na ho?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sonu Nigam","Richa Sharma","Alka Yagnik"]},"style":"rule"},"extra_info":{"index":272,"split":"test"}}
{"id":"test_29","question":"in which sea pearl is found in india?","golden_answers":["the Indian Ocean"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: in which sea pearl is found in india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the Indian Ocean"]},"style":"rule"},"extra_info":{"index":273,"split":"test"}}
{"id":"test_290","question":"who wrote the song mary had a little lamb?","golden_answers":["John Roulstone","Sarah Josepha Hale"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who wrote the song mary had a little lamb?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["John Roulstone","Sarah Josepha Hale"]},"style":"rule"},"extra_info":{"index":274,"split":"test"}}
{"id":"test_2899","question":"what kind of beer is st pauli girl?","golden_answers":["Special Dark","Lager","Non-Alcoholic Malt Beverage"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what kind of beer is st pauli girl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Special Dark","Lager","Non-Alcoholic Malt Beverage"]},"style":"rule"},"extra_info":{"index":275,"split":"test"}}
{"id":"test_2584","question":"mark who went to golf majors in 1998?","golden_answers":["Mark O'Meara","O'Meara"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: mark who went to golf majors in 1998?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mark O'Meara","O'Meara"]},"style":"rule"},"extra_info":{"index":276,"split":"test"}}
{"id":"test_241","question":"who is the voice of the other mother in coraline?","golden_answers":["Teri Hatcher"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the voice of the other mother in coraline?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Teri Hatcher"]},"style":"rule"},"extra_info":{"index":277,"split":"test"}}
{"id":"test_937","question":"when did cybermen first appear in doctor who?","golden_answers":["in 1966","1966"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did cybermen first appear in doctor who?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in 1966","1966"]},"style":"rule"},"extra_info":{"index":278,"split":"test"}}
{"id":"test_276","question":"who played cosette in les miserables on broadway?","golden_answers":["Samantha Hill","Judy Kuhn","Ali Ewoldt"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played cosette in les miserables on broadway?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Samantha Hill","Judy Kuhn","Ali Ewoldt"]},"style":"rule"},"extra_info":{"index":279,"split":"test"}}
{"id":"test_128","question":"who has scored more goals in the premier league?","golden_answers":["Alan Shearer"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has scored more goals in the premier league?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Alan Shearer"]},"style":"rule"},"extra_info":{"index":280,"split":"test"}}
{"id":"test_1353","question":"how many gold medals did australia win in the 2000 olympics?","golden_answers":["16"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many gold medals did australia win in the 2000 olympics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["16"]},"style":"rule"},"extra_info":{"index":281,"split":"test"}}
{"id":"test_3335","question":"nobel laureate who began career as accountant in calcutta?","golden_answers":["Amartya Sen"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: nobel laureate who began career as accountant in calcutta?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Amartya Sen"]},"style":"rule"},"extra_info":{"index":282,"split":"test"}}
{"id":"test_2105","question":"who does eric end up with in that 70s show?","golden_answers":["Donna"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who does eric end up with in that 70s show?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Donna"]},"style":"rule"},"extra_info":{"index":283,"split":"test"}}
{"id":"test_974","question":"who played mrs. trumbull on i love lucy?","golden_answers":["Mary Elizabeth Patterson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played mrs. trumbull on i love lucy?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mary Elizabeth Patterson"]},"style":"rule"},"extra_info":{"index":284,"split":"test"}}
{"id":"test_1140","question":"who plays zoey in i love you man?","golden_answers":["Rashida Jones"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays zoey in i love you man?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rashida Jones"]},"style":"rule"},"extra_info":{"index":285,"split":"test"}}
{"id":"test_2740","question":"who has the most goals in soccer 2018?","golden_answers":["Ali Daei"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most goals in soccer 2018?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ali Daei"]},"style":"rule"},"extra_info":{"index":286,"split":"test"}}
{"id":"test_1988","question":"when did they stop making the nissan xterra?","golden_answers":["2015","after the 2015 model year"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did they stop making the nissan xterra?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2015","after the 2015 model year"]},"style":"rule"},"extra_info":{"index":287,"split":"test"}}
{"id":"test_877","question":"who was the ottoman governor who led egypt in the years following the napoleonic wars?","golden_answers":["Husrev Pasha","Isma'il Pasha and Tewfik Pasha"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the ottoman governor who led egypt in the years following the napoleonic wars?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Husrev Pasha","Isma'il Pasha and Tewfik Pasha"]},"style":"rule"},"extra_info":{"index":288,"split":"test"}}
{"id":"test_3361","question":"who were the twins that played for kentucky?","golden_answers":["Aaron Harrison","Andrew Michael Harrison"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who were the twins that played for kentucky?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Aaron Harrison","Andrew Michael Harrison"]},"style":"rule"},"extra_info":{"index":289,"split":"test"}}
{"id":"test_541","question":"when was the last time dallas cowboys won the super bowl?","golden_answers":["1995"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time dallas cowboys won the super bowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1995"]},"style":"rule"},"extra_info":{"index":290,"split":"test"}}
{"id":"test_2962","question":"where does the this is us family live?","golden_answers":["Pittsburgh"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the this is us family live?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pittsburgh"]},"style":"rule"},"extra_info":{"index":291,"split":"test"}}
{"id":"test_2338","question":"tv show theme song would you like to swing on a star?","golden_answers":["Out of This World"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: tv show theme song would you like to swing on a star?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Out of This World"]},"style":"rule"},"extra_info":{"index":292,"split":"test"}}
{"id":"test_2360","question":"what was the immediate catalyst to the civil war?","golden_answers":["slavery"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the immediate catalyst to the civil war?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["slavery"]},"style":"rule"},"extra_info":{"index":293,"split":"test"}}
{"id":"test_1936","question":"who has the most rings in the nba right now 2017?","golden_answers":["Boston Celtics center Bill Russell","Bill Russell"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most rings in the nba right now 2017?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Boston Celtics center Bill Russell","Bill Russell"]},"style":"rule"},"extra_info":{"index":294,"split":"test"}}
{"id":"test_995","question":"what states do not allow daylight savings time?","golden_answers":["Navajo","Hawaii","Arizona"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what states do not allow daylight savings time?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Navajo","Hawaii","Arizona"]},"style":"rule"},"extra_info":{"index":295,"split":"test"}}
{"id":"test_3213","question":"who played anna in once upon a time?","golden_answers":["Elizabeth Dean Lail"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played anna in once upon a time?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Elizabeth Dean Lail"]},"style":"rule"},"extra_info":{"index":296,"split":"test"}}
{"id":"test_1937","question":"when was the first wonder woman comic released?","golden_answers":["December 1941","January 1942","October 1941"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the first wonder woman comic released?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["December 1941","January 1942","October 1941"]},"style":"rule"},"extra_info":{"index":297,"split":"test"}}
{"id":"test_3307","question":"where can tight junctions be found in the body?","golden_answers":["Internal epithelia","the cytoskeletons of adjacent cells"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where can tight junctions be found in the body?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Internal epithelia","the cytoskeletons of adjacent cells"]},"style":"rule"},"extra_info":{"index":298,"split":"test"}}
{"id":"test_1667","question":"who discovered cells divide to make new cells?","golden_answers":["Hugo von Mohl","German botanist Hugo von Mohl"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who discovered cells divide to make new cells?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Hugo von Mohl","German botanist Hugo von Mohl"]},"style":"rule"},"extra_info":{"index":299,"split":"test"}}
{"id":"test_779","question":"how many countries does cadbury sell its products?","golden_answers":["more than 50 countries worldwide","more than 50"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many countries does cadbury sell its products?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["more than 50 countries worldwide","more than 50"]},"style":"rule"},"extra_info":{"index":300,"split":"test"}}
{"id":"test_386","question":"cast of the have and have nots play?","golden_answers":["Tony Hightower as Frank","Palmer Williams Jr. as Floyd","Maurice Lauchner as Lewis","Jeffery Lewis as Wallie","Alexis Jones as Diane","Kislyck Halsey as Rose","Patrice Lovely as Hattie"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: cast of the have and have nots play?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Tony Hightower as Frank","Palmer Williams Jr. as Floyd","Maurice Lauchner as Lewis","Jeffery Lewis as Wallie","Alexis Jones as Diane","Kislyck Halsey as Rose","Patrice Lovely as Hattie"]},"style":"rule"},"extra_info":{"index":301,"split":"test"}}
{"id":"test_397","question":"how many episodes in great british bake off 2017?","golden_answers":["10"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes in great british bake off 2017?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["10"]},"style":"rule"},"extra_info":{"index":302,"split":"test"}}
{"id":"test_2699","question":"who sings the theme tune to mum on bbc2?","golden_answers":["Lulu and the Lampshades"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the theme tune to mum on bbc2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Lulu and the Lampshades"]},"style":"rule"},"extra_info":{"index":303,"split":"test"}}
{"id":"test_1765","question":"why is the indian ocean the warmest in the world?","golden_answers":["human induced greenhouse warming"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: why is the indian ocean the warmest in the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["human induced greenhouse warming"]},"style":"rule"},"extra_info":{"index":304,"split":"test"}}
{"id":"test_1451","question":"where is season 3 of the detour filmed?","golden_answers":["Alaska"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is season 3 of the detour filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Alaska"]},"style":"rule"},"extra_info":{"index":305,"split":"test"}}
{"id":"test_1734","question":"when did mcgee became a regular on ncis?","golden_answers":["in season two","season two"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did mcgee became a regular on ncis?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["in season two","season two"]},"style":"rule"},"extra_info":{"index":306,"split":"test"}}
{"id":"test_1683","question":"yeh hai mohabbatein serial star cast real name?","golden_answers":["Divyanka Tripathi and Karan Patel"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: yeh hai mohabbatein serial star cast real name?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Divyanka Tripathi and Karan Patel"]},"style":"rule"},"extra_info":{"index":307,"split":"test"}}
{"id":"test_1912","question":"when does the new episodes of supernatural start?","golden_answers":["October\u00a012,\u00a02017","May\u00a03,\u00a02018","October 12, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does the new episodes of supernatural start?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["October\u00a012,\u00a02017","May\u00a03,\u00a02018","October 12, 2017"]},"style":"rule"},"extra_info":{"index":308,"split":"test"}}
{"id":"test_3526","question":"what channel is celebrity big brother on in the usa?","golden_answers":["CBS","on CBS"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what channel is celebrity big brother on in the usa?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["CBS","on CBS"]},"style":"rule"},"extra_info":{"index":309,"split":"test"}}
{"id":"test_221","question":"when was the defensive 3 second rule implemented?","golden_answers":["the 2001\u20132002 season","2001\u20132002 season"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the defensive 3 second rule implemented?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the 2001\u20132002 season","2001\u20132002 season"]},"style":"rule"},"extra_info":{"index":310,"split":"test"}}
{"id":"test_2758","question":"when did ford change the f150 body style?","golden_answers":["the 2009 model year","1957"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did ford change the f150 body style?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the 2009 model year","1957"]},"style":"rule"},"extra_info":{"index":311,"split":"test"}}
{"id":"test_2676","question":"who sings angel of the morning in deadpool?","golden_answers":["Juice Newton","Juice Newton's"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings angel of the morning in deadpool?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Juice Newton","Juice Newton's"]},"style":"rule"},"extra_info":{"index":312,"split":"test"}}
{"id":"test_2646","question":"how many grams of alcohol in one beer?","golden_answers":["14","about 14 grams of alcohol","14 grams"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many grams of alcohol in one beer?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["14","about 14 grams of alcohol","14 grams"]},"style":"rule"},"extra_info":{"index":313,"split":"test"}}
{"id":"test_403","question":"the removal of temperature in fire fighting method is known as?","golden_answers":["cooling","penciling"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the removal of temperature in fire fighting method is known as?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["cooling","penciling"]},"style":"rule"},"extra_info":{"index":314,"split":"test"}}
{"id":"test_248","question":"where was held the first session of muslim league?","golden_answers":["Dhaka, Bangladesh","Lucknow"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was held the first session of muslim league?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Dhaka, Bangladesh","Lucknow"]},"style":"rule"},"extra_info":{"index":315,"split":"test"}}
{"id":"test_1649","question":"where are antibodies made and by what type of lymphocyte?","golden_answers":["B cells","lymph"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where are antibodies made and by what type of lymphocyte?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["B cells","lymph"]},"style":"rule"},"extra_info":{"index":316,"split":"test"}}
{"id":"test_2982","question":"where does trick or treat for unicef money go?","golden_answers":["UNICEF's global programing"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does trick or treat for unicef money go?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["UNICEF's global programing"]},"style":"rule"},"extra_info":{"index":317,"split":"test"}}
{"id":"test_1389","question":"dogs name in the grinch who stole christmas?","golden_answers":["Max"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: dogs name in the grinch who stole christmas?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Max"]},"style":"rule"},"extra_info":{"index":318,"split":"test"}}
{"id":"test_3279","question":"where does the pulmonary trunk receive blood from?","golden_answers":["the right ventricle","from the heart","the heart"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the pulmonary trunk receive blood from?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the right ventricle","from the heart","the heart"]},"style":"rule"},"extra_info":{"index":319,"split":"test"}}
{"id":"test_447","question":"when did the first pair of yeezys come out?","golden_answers":["February 14, 2015"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the first pair of yeezys come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["February 14, 2015"]},"style":"rule"},"extra_info":{"index":320,"split":"test"}}
{"id":"test_1018","question":"who plays the grandmother in game of thrones?","golden_answers":["Rigg"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the grandmother in game of thrones?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rigg"]},"style":"rule"},"extra_info":{"index":321,"split":"test"}}
{"id":"test_784","question":"with a land area of 54 314 square miles where does wisconsin rank among the 50 states?","golden_answers":["25"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: with a land area of 54 314 square miles where does wisconsin rank among the 50 states?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["25"]},"style":"rule"},"extra_info":{"index":322,"split":"test"}}
{"id":"test_3309","question":"where is the tennessee titans football stadium located?","golden_answers":["Nashville, Tennessee"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the tennessee titans football stadium located?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Nashville, Tennessee"]},"style":"rule"},"extra_info":{"index":323,"split":"test"}}
{"id":"test_3547","question":"where was the film the remains of the day filmed?","golden_answers":["Powderham Castle","Weston-super-Mare","Dyrham Park","Badminton House","Corsham Court","Limpley Stoke"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was the film the remains of the day filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Powderham Castle","Weston-super-Mare","Dyrham Park","Badminton House","Corsham Court","Limpley Stoke"]},"style":"rule"},"extra_info":{"index":324,"split":"test"}}
{"id":"test_1837","question":"when does jim propose to pam on the office?","golden_answers":["In the Season 5 premiere"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does jim propose to pam on the office?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["In the Season 5 premiere"]},"style":"rule"},"extra_info":{"index":325,"split":"test"}}
{"id":"test_574","question":"who was the winner of the first indianapolis 500?","golden_answers":["Ray Harroun"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the winner of the first indianapolis 500?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ray Harroun"]},"style":"rule"},"extra_info":{"index":326,"split":"test"}}
{"id":"test_3594","question":"where did the world's largest recorded wave occur?","golden_answers":["Lituya Bay in Alaska"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did the world's largest recorded wave occur?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Lituya Bay in Alaska"]},"style":"rule"},"extra_info":{"index":327,"split":"test"}}
{"id":"test_751","question":"what is the maximum data rate for the 802.11a standard select one?","golden_answers":["54\u00a0Mbit\/s"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the maximum data rate for the 802.11a standard select one?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["54\u00a0Mbit\/s"]},"style":"rule"},"extra_info":{"index":328,"split":"test"}}
{"id":"test_3324","question":"who was the famous scientist that ran the research lab moseley went to in manchester?","golden_answers":["Sir Ernest Rutherford"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the famous scientist that ran the research lab moseley went to in manchester?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sir Ernest Rutherford"]},"style":"rule"},"extra_info":{"index":329,"split":"test"}}
{"id":"test_1894","question":"who has the most wins on around the horn?","golden_answers":["Woody Paige"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most wins on around the horn?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Woody Paige"]},"style":"rule"},"extra_info":{"index":330,"split":"test"}}
{"id":"test_1023","question":"name of black man in to kill a mockingbird?","golden_answers":["Thomas \"Tom\" Robinson","Tom Robinson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: name of black man in to kill a mockingbird?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Thomas \"Tom\" Robinson","Tom Robinson"]},"style":"rule"},"extra_info":{"index":331,"split":"test"}}
{"id":"test_308","question":"when was a series of unfortunate events published?","golden_answers":["September 1999"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was a series of unfortunate events published?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["September 1999"]},"style":"rule"},"extra_info":{"index":332,"split":"test"}}
{"id":"test_1815","question":"tad the lost explorer and the secret of king midas english cast?","golden_answers":["Ariel Winter as Sara Lavrof","Cheech Marin as Freddy","Bruce Mackinnon as the mummy","Lewis MacLeod","Liza Ross as Grandma","Fiona Glascott","Adam Jones as Max Morden"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: tad the lost explorer and the secret of king midas english cast?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ariel Winter as Sara Lavrof","Cheech Marin as Freddy","Bruce Mackinnon as the mummy","Lewis MacLeod","Liza Ross as Grandma","Fiona Glascott","Adam Jones as Max Morden"]},"style":"rule"},"extra_info":{"index":333,"split":"test"}}
{"id":"test_2254","question":"do you cut cards to the left or right?","golden_answers":["right"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: do you cut cards to the left or right?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["right"]},"style":"rule"},"extra_info":{"index":334,"split":"test"}}
{"id":"test_401","question":"when does nathan get in a car accident?","golden_answers":["The Show Must Go On"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does nathan get in a car accident?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["The Show Must Go On"]},"style":"rule"},"extra_info":{"index":335,"split":"test"}}
{"id":"test_207","question":"what position did doug peterson play in the nfl?","golden_answers":["holder on placekicks","quarterback"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what position did doug peterson play in the nfl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["holder on placekicks","quarterback"]},"style":"rule"},"extra_info":{"index":336,"split":"test"}}
{"id":"test_2671","question":"where is fight or flight in the brain?","golden_answers":["the adrenal medulla"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is fight or flight in the brain?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the adrenal medulla"]},"style":"rule"},"extra_info":{"index":337,"split":"test"}}
{"id":"test_2214","question":"when does panic at the disco album come out?","golden_answers":["December 15, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does panic at the disco album come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["December 15, 2017"]},"style":"rule"},"extra_info":{"index":338,"split":"test"}}
{"id":"test_60","question":"who has been ranked no. 1 in the latest football rankings announced by fifa?","golden_answers":["Germany"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has been ranked no. 1 in the latest football rankings announced by fifa?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Germany"]},"style":"rule"},"extra_info":{"index":339,"split":"test"}}
{"id":"test_382","question":"what emperor took over france after the reign of terror?","golden_answers":["Napoleon","Napoleon Bonaparte"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what emperor took over france after the reign of terror?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Napoleon","Napoleon Bonaparte"]},"style":"rule"},"extra_info":{"index":340,"split":"test"}}
{"id":"test_3086","question":"who sang theme song for dukes of hazard?","golden_answers":["Waylon Jennings"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang theme song for dukes of hazard?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Waylon Jennings"]},"style":"rule"},"extra_info":{"index":341,"split":"test"}}
{"id":"test_968","question":"who won the ladies ice skating in the olympics?","golden_answers":["Alina Zagitova"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the ladies ice skating in the olympics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Alina Zagitova"]},"style":"rule"},"extra_info":{"index":342,"split":"test"}}
{"id":"test_681","question":"who is the actor that plays sneaky pete?","golden_answers":["Giovanni Ribisi"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the actor that plays sneaky pete?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Giovanni Ribisi"]},"style":"rule"},"extra_info":{"index":343,"split":"test"}}
{"id":"test_1664","question":"where did the battle of issus take place?","golden_answers":["southern Anatolia","in southern Anatolia"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did the battle of issus take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["southern Anatolia","in southern Anatolia"]},"style":"rule"},"extra_info":{"index":344,"split":"test"}}
{"id":"test_1989","question":"who played the nurse on andy griffith show?","golden_answers":["Julie Adams","Langdon"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played the nurse on andy griffith show?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Julie Adams","Langdon"]},"style":"rule"},"extra_info":{"index":345,"split":"test"}}
{"id":"test_1971","question":"when did rob dyrdek's fantasy factory end?","golden_answers":["March 5, 2015"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did rob dyrdek's fantasy factory end?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["March 5, 2015"]},"style":"rule"},"extra_info":{"index":346,"split":"test"}}
{"id":"test_875","question":"where are trigger points located in the body?","golden_answers":["muscles"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where are trigger points located in the body?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["muscles"]},"style":"rule"},"extra_info":{"index":347,"split":"test"}}
{"id":"test_1642","question":"when does agents of shield season five start?","golden_answers":["December 1, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does agents of shield season five start?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["December 1, 2017"]},"style":"rule"},"extra_info":{"index":348,"split":"test"}}
{"id":"test_240","question":"how many episodes of corrie has there been?","golden_answers":["9,436"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes of corrie has there been?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["9,436"]},"style":"rule"},"extra_info":{"index":349,"split":"test"}}
{"id":"test_674","question":"who discovered that neural communication between cells occurs through chemicals?","golden_answers":["Charles Sherrington"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who discovered that neural communication between cells occurs through chemicals?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Charles Sherrington"]},"style":"rule"},"extra_info":{"index":350,"split":"test"}}
{"id":"test_1552","question":"where is the oldest civilization known to man?","golden_answers":["Mesopotamia"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the oldest civilization known to man?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mesopotamia"]},"style":"rule"},"extra_info":{"index":351,"split":"test"}}
{"id":"test_8","question":"when is the last time the philadelphia won the superbowl?","golden_answers":["Super Bowl LII,","2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is the last time the philadelphia won the superbowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Super Bowl LII,","2017"]},"style":"rule"},"extra_info":{"index":352,"split":"test"}}
{"id":"test_1599","question":"who is the originator of the plan-do-check-act model of performance improvement?","golden_answers":["W. Edwards Deming"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the originator of the plan-do-check-act model of performance improvement?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["W. Edwards Deming"]},"style":"rule"},"extra_info":{"index":353,"split":"test"}}
{"id":"test_1086","question":"who sings the rap in baby by justin bieber?","golden_answers":["Ludacris"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the rap in baby by justin bieber?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ludacris"]},"style":"rule"},"extra_info":{"index":354,"split":"test"}}
{"id":"test_3211","question":"where is the citrus bowl held this year?","golden_answers":["Camping World Stadium"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the citrus bowl held this year?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Camping World Stadium"]},"style":"rule"},"extra_info":{"index":355,"split":"test"}}
{"id":"test_3215","question":"when did american idol end the first time?","golden_answers":["April 7, 2016"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did american idol end the first time?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["April 7, 2016"]},"style":"rule"},"extra_info":{"index":356,"split":"test"}}
{"id":"test_1863","question":"los angeles stadium at hollywood park opening date?","golden_answers":["2020"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: los angeles stadium at hollywood park opening date?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2020"]},"style":"rule"},"extra_info":{"index":357,"split":"test"}}
{"id":"test_1168","question":"what nfl team has the most expensive super bowl ring?","golden_answers":["the New England Patriots","New England Patriots"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what nfl team has the most expensive super bowl ring?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the New England Patriots","New England Patriots"]},"style":"rule"},"extra_info":{"index":358,"split":"test"}}
{"id":"test_1732","question":"who voiced simba in the lion king 2?","golden_answers":["Matthew Broderick"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who voiced simba in the lion king 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Matthew Broderick"]},"style":"rule"},"extra_info":{"index":359,"split":"test"}}
{"id":"test_2853","question":"what type of tale is the pardoner's tale?","golden_answers":["a moral tale","an extended exemplum"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of tale is the pardoner's tale?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["a moral tale","an extended exemplum"]},"style":"rule"},"extra_info":{"index":360,"split":"test"}}
{"id":"test_2992","question":"who lasted the longest in the royal rumble?","golden_answers":["Rey Mysterio"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who lasted the longest in the royal rumble?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rey Mysterio"]},"style":"rule"},"extra_info":{"index":361,"split":"test"}}
{"id":"test_3208","question":"who played nathan scott on one tree hill?","golden_answers":["James Martin Lafferty"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played nathan scott on one tree hill?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["James Martin Lafferty"]},"style":"rule"},"extra_info":{"index":362,"split":"test"}}
{"id":"test_2276","question":"calpurnia son name in to kill a mockingbird?","golden_answers":["Zeebo","James Zeebo"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: calpurnia son name in to kill a mockingbird?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Zeebo","James Zeebo"]},"style":"rule"},"extra_info":{"index":363,"split":"test"}}
{"id":"test_2711","question":"when did the newest macbook pro come out?","golden_answers":["June 5, 2017","October 27, 2016","June\u00a05,\u00a02017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the newest macbook pro come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["June 5, 2017","October 27, 2016","June\u00a05,\u00a02017"]},"style":"rule"},"extra_info":{"index":364,"split":"test"}}
{"id":"test_2942","question":"when was nepal declared a secular state in bs?","golden_answers":["January 15, 2007"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was nepal declared a secular state in bs?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["January 15, 2007"]},"style":"rule"},"extra_info":{"index":365,"split":"test"}}
{"id":"test_1993","question":"who headed the 7th central pay commission of india?","golden_answers":["Justice A.K Mathur"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who headed the 7th central pay commission of india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Justice A.K Mathur"]},"style":"rule"},"extra_info":{"index":366,"split":"test"}}
{"id":"test_634","question":"total goals scored by ronaldo in la liga?","golden_answers":["309"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: total goals scored by ronaldo in la liga?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["309"]},"style":"rule"},"extra_info":{"index":367,"split":"test"}}
{"id":"test_777","question":"in photosynthesis the carbon in co2 is initially fixed to what molecule?","golden_answers":["3-phosphoglycerate"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: in photosynthesis the carbon in co2 is initially fixed to what molecule?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["3-phosphoglycerate"]},"style":"rule"},"extra_info":{"index":368,"split":"test"}}
{"id":"test_1215","question":"where does cerebrospinal fluid flow to when it exits the cerebral aqueduct?","golden_answers":["the fourth ventricle","fourth ventricle"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does cerebrospinal fluid flow to when it exits the cerebral aqueduct?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the fourth ventricle","fourth ventricle"]},"style":"rule"},"extra_info":{"index":369,"split":"test"}}
{"id":"test_891","question":"what is the most famous building in rennes?","golden_answers":["Parlement de Bretagne","The Parlement de Bretagne"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the most famous building in rennes?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Parlement de Bretagne","The Parlement de Bretagne"]},"style":"rule"},"extra_info":{"index":370,"split":"test"}}
{"id":"test_239","question":"panic at the disco song about a wedding?","golden_answers":["I Write Sins Not Tragedies"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: panic at the disco song about a wedding?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["I Write Sins Not Tragedies"]},"style":"rule"},"extra_info":{"index":371,"split":"test"}}
{"id":"test_2372","question":"what was the meaning of the song puff the magic dragon?","golden_answers":["the hardships of growing older"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what was the meaning of the song puff the magic dragon?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the hardships of growing older"]},"style":"rule"},"extra_info":{"index":372,"split":"test"}}
{"id":"test_3013","question":"what type of writing did ancient egypt use?","golden_answers":["hieroglyphs","Egyptian hieroglyphs"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of writing did ancient egypt use?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["hieroglyphs","Egyptian hieroglyphs"]},"style":"rule"},"extra_info":{"index":373,"split":"test"}}
{"id":"test_2220","question":"what college does everyone in gossip girl go to?","golden_answers":["New York University","Columbia University"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what college does everyone in gossip girl go to?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["New York University","Columbia University"]},"style":"rule"},"extra_info":{"index":374,"split":"test"}}
{"id":"test_249","question":"what is the name of the dragon in eragon?","golden_answers":["Saphira"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the name of the dragon in eragon?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Saphira"]},"style":"rule"},"extra_info":{"index":375,"split":"test"}}
{"id":"test_3063","question":"who is ishani in lies of the heart?","golden_answers":["Siddharth Arora\/Vibhav Roy","Nalini Negi"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is ishani in lies of the heart?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Siddharth Arora\/Vibhav Roy","Nalini Negi"]},"style":"rule"},"extra_info":{"index":376,"split":"test"}}
{"id":"test_1284","question":"when is i can only imagine coming out?","golden_answers":["March 16, 2018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is i can only imagine coming out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["March 16, 2018"]},"style":"rule"},"extra_info":{"index":377,"split":"test"}}
{"id":"test_234","question":"who sings that aint no way to go?","golden_answers":["Brooks & Dunn"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings that aint no way to go?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Brooks & Dunn"]},"style":"rule"},"extra_info":{"index":378,"split":"test"}}
{"id":"test_205","question":"method used by a writer to develop a character?","golden_answers":["Anthropomorphism","Personification","Hamartia","Pathetic fallacy"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: method used by a writer to develop a character?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Anthropomorphism","Personification","Hamartia","Pathetic fallacy"]},"style":"rule"},"extra_info":{"index":379,"split":"test"}}
{"id":"test_2392","question":"what category was hurricane charley when it hit florida?","golden_answers":["4","Category 4","Category\u00a04"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what category was hurricane charley when it hit florida?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["4","Category 4","Category\u00a04"]},"style":"rule"},"extra_info":{"index":380,"split":"test"}}
{"id":"test_1952","question":"how long is a prime minister term in uk?","golden_answers":["At Her Majesty's pleasure"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how long is a prime minister term in uk?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["At Her Majesty's pleasure"]},"style":"rule"},"extra_info":{"index":381,"split":"test"}}
{"id":"test_2059","question":"who sings the wire season 5 theme song?","golden_answers":["Steve Earle"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the wire season 5 theme song?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Steve Earle"]},"style":"rule"},"extra_info":{"index":382,"split":"test"}}
{"id":"test_2175","question":"who has participated in the most super bowls?","golden_answers":["New England Patriots"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has participated in the most super bowls?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["New England Patriots"]},"style":"rule"},"extra_info":{"index":383,"split":"test"}}
{"id":"test_644","question":"who sings somebody's watching me with michael jackson?","golden_answers":["Jermaine Jackson","Rockwell"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings somebody's watching me with michael jackson?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jermaine Jackson","Rockwell"]},"style":"rule"},"extra_info":{"index":384,"split":"test"}}
{"id":"test_232","question":"what nfl team is robert griffin the third playing for?","golden_answers":["currently a free agent"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what nfl team is robert griffin the third playing for?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["currently a free agent"]},"style":"rule"},"extra_info":{"index":385,"split":"test"}}
{"id":"test_2080","question":"when does season 13 of america's got talent premiere?","golden_answers":["May 29, 2018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does season 13 of america's got talent premiere?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["May 29, 2018"]},"style":"rule"},"extra_info":{"index":386,"split":"test"}}
{"id":"test_328","question":"where does a wrinkle in time take place?","golden_answers":["Connecticut"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does a wrinkle in time take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Connecticut"]},"style":"rule"},"extra_info":{"index":387,"split":"test"}}
{"id":"test_761","question":"how old was sasuke when his clan died?","golden_answers":["seven"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how old was sasuke when his clan died?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["seven"]},"style":"rule"},"extra_info":{"index":388,"split":"test"}}
{"id":"test_280","question":"what type of plate boundary is associated with iceland and its volcanic eruptions?","golden_answers":["divergent tectonic plate boundary","a divergent tectonic plate boundary","the mid-Atlantic Ridge"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of plate boundary is associated with iceland and its volcanic eruptions?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["divergent tectonic plate boundary","a divergent tectonic plate boundary","the mid-Atlantic Ridge"]},"style":"rule"},"extra_info":{"index":389,"split":"test"}}
{"id":"test_2437","question":"who won the most on who wants to be a millionaire?","golden_answers":["Kevin Olmstead","David Goodman"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the most on who wants to be a millionaire?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kevin Olmstead","David Goodman"]},"style":"rule"},"extra_info":{"index":390,"split":"test"}}
{"id":"test_278","question":"who ran the fastest 40 yard dash in the nfl?","golden_answers":["Jakeem Grant","John Ross"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who ran the fastest 40 yard dash in the nfl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jakeem Grant","John Ross"]},"style":"rule"},"extra_info":{"index":391,"split":"test"}}
{"id":"test_2765","question":"who was the guy who died in glee?","golden_answers":["Cory Allan Michael Monteith"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the guy who died in glee?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Cory Allan Michael Monteith"]},"style":"rule"},"extra_info":{"index":392,"split":"test"}}
{"id":"test_963","question":"what is the third season of total drama?","golden_answers":["World Tour","Total Drama World Tour"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the third season of total drama?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["World Tour","Total Drama World Tour"]},"style":"rule"},"extra_info":{"index":393,"split":"test"}}
{"id":"test_1653","question":"where is the world's largest thermometer located?","golden_answers":["Baker, California, USA","Baker, California"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where is the world's largest thermometer located?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Baker, California, USA","Baker, California"]},"style":"rule"},"extra_info":{"index":394,"split":"test"}}
{"id":"test_491","question":"who was the first to say i'm going to disney world?","golden_answers":["Jeana Yeager","Phil Simms","Dick Rutan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the first to say i'm going to disney world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jeana Yeager","Phil Simms","Dick Rutan"]},"style":"rule"},"extra_info":{"index":395,"split":"test"}}
{"id":"test_2333","question":"where did the tea come from in the boston tea party?","golden_answers":["England","East India Company","the East India Company"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where did the tea come from in the boston tea party?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["England","East India Company","the East India Company"]},"style":"rule"},"extra_info":{"index":396,"split":"test"}}
{"id":"test_1008","question":"new movie of ajay devgan and sonakshi sinha?","golden_answers":["Action Jackson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: new movie of ajay devgan and sonakshi sinha?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Action Jackson"]},"style":"rule"},"extra_info":{"index":397,"split":"test"}}
{"id":"test_2371","question":"who sings i feel love with the blue man group?","golden_answers":["Annette Strean","Annette","Venus Hum"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings i feel love with the blue man group?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Annette Strean","Annette","Venus Hum"]},"style":"rule"},"extra_info":{"index":398,"split":"test"}}
{"id":"test_2435","question":"what type of artwork was created in the safavid empire?","golden_answers":["architecture","metal","ceramics","gardens","book","glass"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of artwork was created in the safavid empire?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["architecture","metal","ceramics","gardens","book","glass"]},"style":"rule"},"extra_info":{"index":399,"split":"test"}}
{"id":"test_162","question":"who want to be a millionaire calls his dad?","golden_answers":["Carpenter"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who want to be a millionaire calls his dad?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Carpenter"]},"style":"rule"},"extra_info":{"index":400,"split":"test"}}
{"id":"test_2536","question":"where does the brazos river start and stop?","golden_answers":["Gulf of Mexico","Llano Estacado"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the brazos river start and stop?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gulf of Mexico","Llano Estacado"]},"style":"rule"},"extra_info":{"index":401,"split":"test"}}
{"id":"test_335","question":"how many episodes in season 3 of good witch?","golden_answers":["10"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes in season 3 of good witch?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["10"]},"style":"rule"},"extra_info":{"index":402,"split":"test"}}
{"id":"test_1717","question":"who sang gonna sit right down and write myself a letter?","golden_answers":["Fats Waller"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang gonna sit right down and write myself a letter?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Fats Waller"]},"style":"rule"},"extra_info":{"index":403,"split":"test"}}
{"id":"test_2692","question":"who inaugurated 'world teachers' day'?","golden_answers":["UNESCO","ILO"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who inaugurated 'world teachers' day'?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["UNESCO","ILO"]},"style":"rule"},"extra_info":{"index":404,"split":"test"}}
{"id":"test_3458","question":"what is the common name for gravitational force?","golden_answers":["Gravity","Gravity, or gravitation"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the common name for gravitational force?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Gravity","Gravity, or gravitation"]},"style":"rule"},"extra_info":{"index":405,"split":"test"}}
{"id":"test_2315","question":"which body part(s) occupy the greatest portion of the primary motor cortex?","golden_answers":["face","the human hands"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which body part(s) occupy the greatest portion of the primary motor cortex?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["face","the human hands"]},"style":"rule"},"extra_info":{"index":406,"split":"test"}}
{"id":"test_2141","question":"who do you meet at the gates of heaven?","golden_answers":["Saint Peter"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who do you meet at the gates of heaven?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Saint Peter"]},"style":"rule"},"extra_info":{"index":407,"split":"test"}}
{"id":"test_1295","question":"when was i can only imagine the song released?","golden_answers":["1999","2001"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was i can only imagine the song released?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1999","2001"]},"style":"rule"},"extra_info":{"index":408,"split":"test"}}
{"id":"test_1068","question":"who made the song we are the world?","golden_answers":["produced by Quincy Jones"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who made the song we are the world?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["produced by Quincy Jones"]},"style":"rule"},"extra_info":{"index":409,"split":"test"}}
{"id":"test_836","question":"when was the death penalty reinstated in oregon?","golden_answers":["1984"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the death penalty reinstated in oregon?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1984"]},"style":"rule"},"extra_info":{"index":410,"split":"test"}}
{"id":"test_2743","question":"where does a brisket come from on a cow?","golden_answers":["the breast or lower chest","breast or lower chest"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does a brisket come from on a cow?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the breast or lower chest","breast or lower chest"]},"style":"rule"},"extra_info":{"index":411,"split":"test"}}
{"id":"test_2933","question":"which country is the last member of saarc?","golden_answers":["Afghanistan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which country is the last member of saarc?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Afghanistan"]},"style":"rule"},"extra_info":{"index":412,"split":"test"}}
{"id":"test_1286","question":"who has the most conference championships in college basketball?","golden_answers":["Kansas"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has the most conference championships in college basketball?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kansas"]},"style":"rule"},"extra_info":{"index":413,"split":"test"}}
{"id":"test_977","question":"what is the order of the netflix marvel shows?","golden_answers":["Marvel's Iron Fist","Marvel's Daredevil","Marvel's The Punisher","Marvel's Jessica Jones","Marvel's The Defenders","Marvel's Luke Cage"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the order of the netflix marvel shows?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Marvel's Iron Fist","Marvel's Daredevil","Marvel's The Punisher","Marvel's Jessica Jones","Marvel's The Defenders","Marvel's Luke Cage"]},"style":"rule"},"extra_info":{"index":414,"split":"test"}}
{"id":"test_1087","question":"who plays the beast on the new beauty and the beast?","golden_answers":["Dan Stevens"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays the beast on the new beauty and the beast?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Dan Stevens"]},"style":"rule"},"extra_info":{"index":415,"split":"test"}}
{"id":"test_1621","question":"who made the first to record with the electric guitar?","golden_answers":["George Warren Barnes"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who made the first to record with the electric guitar?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["George Warren Barnes"]},"style":"rule"},"extra_info":{"index":416,"split":"test"}}
{"id":"test_536","question":"skin that covers the palms fingertips and soles of the feet?","golden_answers":["stratum lucidum"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: skin that covers the palms fingertips and soles of the feet?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["stratum lucidum"]},"style":"rule"},"extra_info":{"index":417,"split":"test"}}
{"id":"test_3353","question":"who used the word physiology for the first time?","golden_answers":["Jean Fernel"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who used the word physiology for the first time?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jean Fernel"]},"style":"rule"},"extra_info":{"index":418,"split":"test"}}
{"id":"test_2643","question":"what type of boundary was the mexico earthquake?","golden_answers":["a subduction zone"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of boundary was the mexico earthquake?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["a subduction zone"]},"style":"rule"},"extra_info":{"index":419,"split":"test"}}
{"id":"test_1228","question":"who sang what i like about you originally?","golden_answers":["The Romantics.","The Romantics","American rock band The Romantics"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang what i like about you originally?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["The Romantics.","The Romantics","American rock band The Romantics"]},"style":"rule"},"extra_info":{"index":420,"split":"test"}}
{"id":"test_1872","question":"the first significant restriction on free immigration in u.s. history was the?","golden_answers":["Naturalization Act of 1790","Chinese Exclusion Act in 1882"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the first significant restriction on free immigration in u.s. history was the?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Naturalization Act of 1790","Chinese Exclusion Act in 1882"]},"style":"rule"},"extra_info":{"index":421,"split":"test"}}
{"id":"test_3201","question":"who is opening for little mix glory days tour?","golden_answers":["Sheppard","Zoe Badwi","Louisa Johnson","Bronnie","The Vamps","Ella Eyre","Conor Maynard"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is opening for little mix glory days tour?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sheppard","Zoe Badwi","Louisa Johnson","Bronnie","The Vamps","Ella Eyre","Conor Maynard"]},"style":"rule"},"extra_info":{"index":422,"split":"test"}}
{"id":"test_3079","question":"what is the emblematic rhythm of dominican republic?","golden_answers":["merengue","bachata"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the emblematic rhythm of dominican republic?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["merengue","bachata"]},"style":"rule"},"extra_info":{"index":423,"split":"test"}}
{"id":"test_297","question":"when was i look at the world poem written?","golden_answers":["30\u201331 October 2000"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was i look at the world poem written?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["30\u201331 October 2000"]},"style":"rule"},"extra_info":{"index":424,"split":"test"}}
{"id":"test_38","question":"who is rose in the fall season 2?","golden_answers":["Valene Kane"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is rose in the fall season 2?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Valene Kane"]},"style":"rule"},"extra_info":{"index":425,"split":"test"}}
{"id":"test_1877","question":"who was the first english child born in north america?","golden_answers":["Virginia Dare"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who was the first english child born in north america?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Virginia Dare"]},"style":"rule"},"extra_info":{"index":426,"split":"test"}}
{"id":"test_2544","question":"what nfl coach has the most wins ever?","golden_answers":["Shula, Don","Don Shula"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what nfl coach has the most wins ever?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Shula, Don","Don Shula"]},"style":"rule"},"extra_info":{"index":427,"split":"test"}}
{"id":"test_2306","question":"who proposed evolution in 1859 as the basis of biological development?","golden_answers":["Alfred Russel Wallace","Charles Darwin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who proposed evolution in 1859 as the basis of biological development?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Alfred Russel Wallace","Charles Darwin"]},"style":"rule"},"extra_info":{"index":428,"split":"test"}}
{"id":"test_409","question":"ganglion axons forming the optic nerve run to the?","golden_answers":["pretectal nucleus","suprachiasmatic nucleus","photoreceptor cells","lateral geniculate nucleus","optic chiasma"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: ganglion axons forming the optic nerve run to the?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["pretectal nucleus","suprachiasmatic nucleus","photoreceptor cells","lateral geniculate nucleus","optic chiasma"]},"style":"rule"},"extra_info":{"index":429,"split":"test"}}
{"id":"test_300","question":"who kills barry's mom in the flash?","golden_answers":["Professor Eobard Thawne","Thawne"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who kills barry's mom in the flash?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Professor Eobard Thawne","Thawne"]},"style":"rule"},"extra_info":{"index":430,"split":"test"}}
{"id":"test_2202","question":"where does the donkey talk in the bible?","golden_answers":["Numbers 22:28"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the donkey talk in the bible?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Numbers 22:28"]},"style":"rule"},"extra_info":{"index":431,"split":"test"}}
{"id":"test_873","question":"who is the valley of the dolls based on?","golden_answers":["Dean Martin","Judy Garland","Ethel Merman","Carole Landis"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the valley of the dolls based on?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Dean Martin","Judy Garland","Ethel Merman","Carole Landis"]},"style":"rule"},"extra_info":{"index":432,"split":"test"}}
{"id":"test_2072","question":"when was the first super bowl the eagles ever won?","golden_answers":["2017","1948"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the first super bowl the eagles ever won?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2017","1948"]},"style":"rule"},"extra_info":{"index":433,"split":"test"}}
{"id":"test_3255","question":"who played tom in four weddings and a funeral?","golden_answers":["James Fleet"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played tom in four weddings and a funeral?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["James Fleet"]},"style":"rule"},"extra_info":{"index":434,"split":"test"}}
{"id":"test_542","question":"where does hydrogen peroxide come from in the body?","golden_answers":["nearly all living cells"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does hydrogen peroxide come from in the body?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["nearly all living cells"]},"style":"rule"},"extra_info":{"index":435,"split":"test"}}
{"id":"test_1429","question":"perth is the capital of which australian state?","golden_answers":["Western Australia"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: perth is the capital of which australian state?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Western Australia"]},"style":"rule"},"extra_info":{"index":436,"split":"test"}}
{"id":"test_281","question":"who plays jimmy's mom in 8 mile?","golden_answers":["Kim Basinger"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays jimmy's mom in 8 mile?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Kim Basinger"]},"style":"rule"},"extra_info":{"index":437,"split":"test"}}
{"id":"test_1000","question":"when did the nba add the three point line?","golden_answers":["1979\u201380","1979\u201380 season","the 1979\u201380 season"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the nba add the three point line?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1979\u201380","1979\u201380 season","the 1979\u201380 season"]},"style":"rule"},"extra_info":{"index":438,"split":"test"}}
{"id":"test_1513","question":"what year does the quiet man take place?","golden_answers":["the 1920s","In the 1920s"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what year does the quiet man take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the 1920s","In the 1920s"]},"style":"rule"},"extra_info":{"index":439,"split":"test"}}
{"id":"test_1167","question":"who won women's singles us open 2017?","golden_answers":["Sloane Stephens"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won women's singles us open 2017?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sloane Stephens"]},"style":"rule"},"extra_info":{"index":440,"split":"test"}}
{"id":"test_646","question":"when does the second part of vikings season 4 start?","golden_answers":["November 30, 2016"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does the second part of vikings season 4 start?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["November 30, 2016"]},"style":"rule"},"extra_info":{"index":441,"split":"test"}}
{"id":"test_3350","question":"who won the ncaa basketball championship in 1994?","golden_answers":["Arkansas","Arkansas Razorbacks"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the ncaa basketball championship in 1994?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Arkansas","Arkansas Razorbacks"]},"style":"rule"},"extra_info":{"index":442,"split":"test"}}
{"id":"test_2225","question":"when is the last time the dolphins went to the superbowl?","golden_answers":["Super Bowl XIX","1984"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is the last time the dolphins went to the superbowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Super Bowl XIX","1984"]},"style":"rule"},"extra_info":{"index":443,"split":"test"}}
{"id":"test_2881","question":"what are the spices in chinese 5 spice powder?","golden_answers":["Sichuan pepper","Cloves (dingxiang \u4e01\u9999)","Star anise","Cloves","Chinese cinnamon","Fennel seeds"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what are the spices in chinese 5 spice powder?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Sichuan pepper","Cloves (dingxiang \u4e01\u9999)","Star anise","Cloves","Chinese cinnamon","Fennel seeds"]},"style":"rule"},"extra_info":{"index":444,"split":"test"}}
{"id":"test_1239","question":"who won the 2017 ncaa mens basketball tournament?","golden_answers":["North Carolina"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who won the 2017 ncaa mens basketball tournament?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["North Carolina"]},"style":"rule"},"extra_info":{"index":445,"split":"test"}}
{"id":"test_2505","question":"when did the cubs won a world series?","golden_answers":["1908","1907","2016"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the cubs won a world series?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1908","1907","2016"]},"style":"rule"},"extra_info":{"index":446,"split":"test"}}
{"id":"test_2678","question":"when is the new series of keeping faith starting?","golden_answers":["13 February 2018"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when is the new series of keeping faith starting?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["13 February 2018"]},"style":"rule"},"extra_info":{"index":447,"split":"test"}}
{"id":"test_2166","question":"what is the longest panic at the disco song title?","golden_answers":["Bohemian Rhapsody"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the longest panic at the disco song title?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Bohemian Rhapsody"]},"style":"rule"},"extra_info":{"index":448,"split":"test"}}
{"id":"test_32","question":"what type of car is a jeep considered?","golden_answers":["off-road vehicles","light utility vehicles","sport utility vehicles"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what type of car is a jeep considered?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["off-road vehicles","light utility vehicles","sport utility vehicles"]},"style":"rule"},"extra_info":{"index":449,"split":"test"}}
{"id":"test_2735","question":"who is playing halftime at the pro bowl?","golden_answers":["Jordan Fisher"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is playing halftime at the pro bowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jordan Fisher"]},"style":"rule"},"extra_info":{"index":450,"split":"test"}}
{"id":"test_2271","question":"ranjit sagar dam has builded over which river?","golden_answers":["Ravi River","the Ravi River"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: ranjit sagar dam has builded over which river?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Ravi River","the Ravi River"]},"style":"rule"},"extra_info":{"index":451,"split":"test"}}
{"id":"test_1226","question":"who played mike stivic on all in the family?","golden_answers":["Rob Reiner"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played mike stivic on all in the family?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Rob Reiner"]},"style":"rule"},"extra_info":{"index":452,"split":"test"}}
{"id":"test_2717","question":"what is the account number of airtel payment bank?","golden_answers":["Your Airtel mobile number"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the account number of airtel payment bank?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Your Airtel mobile number"]},"style":"rule"},"extra_info":{"index":453,"split":"test"}}
{"id":"test_424","question":"where does the paraguay river start and end?","golden_answers":["Paran\u00e1 River"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the paraguay river start and end?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Paran\u00e1 River"]},"style":"rule"},"extra_info":{"index":454,"split":"test"}}
{"id":"test_550","question":"who developed the central processing unit (cpu)?","golden_answers":["John von Neumann"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who developed the central processing unit (cpu)?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["John von Neumann"]},"style":"rule"},"extra_info":{"index":455,"split":"test"}}
{"id":"test_3552","question":"puella magi madoka magica when does madoka become a magical girl?","golden_answers":["My Very Best Friend"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: puella magi madoka magica when does madoka become a magical girl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["My Very Best Friend"]},"style":"rule"},"extra_info":{"index":456,"split":"test"}}
{"id":"test_472","question":"who is mowgli's main enemy in the jungle book?","golden_answers":["Shere Khan"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is mowgli's main enemy in the jungle book?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Shere Khan"]},"style":"rule"},"extra_info":{"index":457,"split":"test"}}
{"id":"test_438","question":"number of employees in the department of health and human services?","golden_answers":["79,540 (2015)","79,540"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: number of employees in the department of health and human services?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["79,540 (2015)","79,540"]},"style":"rule"},"extra_info":{"index":458,"split":"test"}}
{"id":"test_3040","question":"who is the quarterback for the green bay packers?","golden_answers":["Aaron Rodgers"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the quarterback for the green bay packers?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Aaron Rodgers"]},"style":"rule"},"extra_info":{"index":459,"split":"test"}}
{"id":"test_3433","question":"who played young clark kent in 1978 superman?","golden_answers":["Jeff East"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played young clark kent in 1978 superman?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Jeff East"]},"style":"rule"},"extra_info":{"index":460,"split":"test"}}
{"id":"test_3564","question":"who ruled the ottoman empire in the 1500s?","golden_answers":["Selim I"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who ruled the ottoman empire in the 1500s?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Selim I"]},"style":"rule"},"extra_info":{"index":461,"split":"test"}}
{"id":"test_1115","question":"when does dragon ball super episode 113 start?","golden_answers":["October 29, 2017"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when does dragon ball super episode 113 start?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["October 29, 2017"]},"style":"rule"},"extra_info":{"index":462,"split":"test"}}
{"id":"test_1154","question":"the most common form of megalithic architecture in europe is?","golden_answers":["the portal tomb","portal tomb"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: the most common form of megalithic architecture in europe is?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["the portal tomb","portal tomb"]},"style":"rule"},"extra_info":{"index":463,"split":"test"}}
{"id":"test_2477","question":"when did drop it like it's hot release?","golden_answers":["September 12, 2004"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did drop it like it's hot release?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["September 12, 2004"]},"style":"rule"},"extra_info":{"index":464,"split":"test"}}
{"id":"test_862","question":"who is the present rajya sabha speaker of india?","golden_answers":["Venkaiah Naidu"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who is the present rajya sabha speaker of india?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Venkaiah Naidu"]},"style":"rule"},"extra_info":{"index":465,"split":"test"}}
{"id":"test_2939","question":"who plays chummy's mother in call the midwife?","golden_answers":["Cheryl Campbell"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays chummy's mother in call the midwife?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Cheryl Campbell"]},"style":"rule"},"extra_info":{"index":466,"split":"test"}}
{"id":"test_1404","question":"how many episodes is season 4 of the flash?","golden_answers":["23 episodes","19","23"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes is season 4 of the flash?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["23 episodes","19","23"]},"style":"rule"},"extra_info":{"index":467,"split":"test"}}
{"id":"test_833","question":"which domain of life are humans members of?","golden_answers":["Eukarya"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which domain of life are humans members of?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Eukarya"]},"style":"rule"},"extra_info":{"index":468,"split":"test"}}
{"id":"test_2815","question":"when did australia 2 win the america's cup?","golden_answers":["1983"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did australia 2 win the america's cup?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1983"]},"style":"rule"},"extra_info":{"index":469,"split":"test"}}
{"id":"test_2597","question":"how many times has lake placid hosted the winter olympics?","golden_answers":["twice"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many times has lake placid hosted the winter olympics?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["twice"]},"style":"rule"},"extra_info":{"index":470,"split":"test"}}
{"id":"test_1081","question":"who sang once upon a dream at the end of maleficent?","golden_answers":["Lana Del Rey"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang once upon a dream at the end of maleficent?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Lana Del Rey"]},"style":"rule"},"extra_info":{"index":471,"split":"test"}}
{"id":"test_2070","question":"where does half life 2 episode 2 take place?","golden_answers":["outside City 17"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does half life 2 episode 2 take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["outside City 17"]},"style":"rule"},"extra_info":{"index":472,"split":"test"}}
{"id":"test_3391","question":"who sang national anthem at the super bowl?","golden_answers":["Pink"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang national anthem at the super bowl?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pink"]},"style":"rule"},"extra_info":{"index":473,"split":"test"}}
{"id":"test_1028","question":"where was it happened at the world fair filmed?","golden_answers":["Seattle, Washington","The Seattle Center","Camarillo, California"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where was it happened at the world fair filmed?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Seattle, Washington","The Seattle Center","Camarillo, California"]},"style":"rule"},"extra_info":{"index":474,"split":"test"}}
{"id":"test_208","question":"who played lionel in as time goes by?","golden_answers":["Geoffrey Dyson Palmer, OBE","Geoffrey Dyson Palmer"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played lionel in as time goes by?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Geoffrey Dyson Palmer, OBE","Geoffrey Dyson Palmer"]},"style":"rule"},"extra_info":{"index":475,"split":"test"}}
{"id":"test_378","question":"who sang i'm gonna run away from you?","golden_answers":["Tami Lynn"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sang i'm gonna run away from you?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Tami Lynn"]},"style":"rule"},"extra_info":{"index":476,"split":"test"}}
{"id":"test_2598","question":"who came up with the theory of relativity?","golden_answers":["Albert Einstein"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who came up with the theory of relativity?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Albert Einstein"]},"style":"rule"},"extra_info":{"index":477,"split":"test"}}
{"id":"test_3303","question":"when did the ouija board first come out?","golden_answers":["July 1, 1890"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the ouija board first come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["July 1, 1890"]},"style":"rule"},"extra_info":{"index":478,"split":"test"}}
{"id":"test_1133","question":"what is the meaning of the word autumn?","golden_answers":["fall"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the meaning of the word autumn?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["fall"]},"style":"rule"},"extra_info":{"index":479,"split":"test"}}
{"id":"test_180","question":"industrial city in germany on the rhine herne canal?","golden_answers":["Henrichenburg","Duisburg"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: industrial city in germany on the rhine herne canal?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Henrichenburg","Duisburg"]},"style":"rule"},"extra_info":{"index":480,"split":"test"}}
{"id":"test_14","question":"who designed the garden city of new earswick?","golden_answers":["planner Raymond Unwin","architect Barry Parker","Raymond Unwin"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who designed the garden city of new earswick?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["planner Raymond Unwin","architect Barry Parker","Raymond Unwin"]},"style":"rule"},"extra_info":{"index":481,"split":"test"}}
{"id":"test_1366","question":"where can carbon be found in the biosphere?","golden_answers":["other living organisms","The terrestrial biosphere","plants","soil"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where can carbon be found in the biosphere?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["other living organisms","The terrestrial biosphere","plants","soil"]},"style":"rule"},"extra_info":{"index":482,"split":"test"}}
{"id":"test_535","question":"who sings the song i'll never forget you?","golden_answers":["Mariah Carey","Zara Larsson and MNEK","Noisettes"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who sings the song i'll never forget you?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Mariah Carey","Zara Larsson and MNEK","Noisettes"]},"style":"rule"},"extra_info":{"index":483,"split":"test"}}
{"id":"test_2609","question":"who made the movie fifty shades of grey?","golden_answers":["Universal Pictures and Focus Features","Sam Taylor-Johnson"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who made the movie fifty shades of grey?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Universal Pictures and Focus Features","Sam Taylor-Johnson"]},"style":"rule"},"extra_info":{"index":484,"split":"test"}}
{"id":"test_1072","question":"when was the last time the dodgers played yankees in the world series?","golden_answers":["1981"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the last time the dodgers played yankees in the world series?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1981"]},"style":"rule"},"extra_info":{"index":485,"split":"test"}}
{"id":"test_3398","question":"who plays nikko in the wizard of oz?","golden_answers":["Patrick Walshe"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who plays nikko in the wizard of oz?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Patrick Walshe"]},"style":"rule"},"extra_info":{"index":486,"split":"test"}}
{"id":"test_3036","question":"how many episodes curse of oak island season 5?","golden_answers":["18"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many episodes curse of oak island season 5?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["18"]},"style":"rule"},"extra_info":{"index":487,"split":"test"}}
{"id":"test_1809","question":"how many countries participated for the first time in the 2014 olympic winter games in sochi?","golden_answers":["Brazil","Uzbekistan","Turkey"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how many countries participated for the first time in the 2014 olympic winter games in sochi?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Brazil","Uzbekistan","Turkey"]},"style":"rule"},"extra_info":{"index":488,"split":"test"}}
{"id":"test_2259","question":"who played big enos in smokey and the bandit?","golden_answers":["Pat McCormick"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who played big enos in smokey and the bandit?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Pat McCormick"]},"style":"rule"},"extra_info":{"index":489,"split":"test"}}
{"id":"test_2890","question":"what is the movie about six degrees of separation?","golden_answers":["Six Degrees of Separation"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: what is the movie about six degrees of separation?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Six Degrees of Separation"]},"style":"rule"},"extra_info":{"index":490,"split":"test"}}
{"id":"test_1751","question":"which supreme court judge has surved in international court of justice?","golden_answers":["Dalveer Bhandari"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: which supreme court judge has surved in international court of justice?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Dalveer Bhandari"]},"style":"rule"},"extra_info":{"index":491,"split":"test"}}
{"id":"test_2297","question":"when was the $1 000 bill discontinued?","golden_answers":["1969","December 27, 1945","July 14, 1969"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when was the $1 000 bill discontinued?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1969","December 27, 1945","July 14, 1969"]},"style":"rule"},"extra_info":{"index":492,"split":"test"}}
{"id":"test_39","question":"where does the story the great gatsby take place?","golden_answers":["Long Island of 1922"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: where does the story the great gatsby take place?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Long Island of 1922"]},"style":"rule"},"extra_info":{"index":493,"split":"test"}}
{"id":"test_3389","question":"when did lionel messi play his first game for barcelona?","golden_answers":["2002","2001","October 2004"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did lionel messi play his first game for barcelona?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["2002","2001","October 2004"]},"style":"rule"},"extra_info":{"index":494,"split":"test"}}
{"id":"test_3277","question":"how do you say evil eye in greek?","golden_answers":["matiasma","vaskania (\u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1)"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: how do you say evil eye in greek?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["matiasma","vaskania (\u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1)"]},"style":"rule"},"extra_info":{"index":495,"split":"test"}}
{"id":"test_2830","question":"who discovered one of the first taxonomic classification schemes?","golden_answers":["Swedish botanist Carl Linnaeus"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who discovered one of the first taxonomic classification schemes?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Swedish botanist Carl Linnaeus"]},"style":"rule"},"extra_info":{"index":496,"split":"test"}}
{"id":"test_610","question":"when did teenage mutant ninja turtles come out?","golden_answers":["1984"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did teenage mutant ninja turtles come out?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["1984"]},"style":"rule"},"extra_info":{"index":497,"split":"test"}}
{"id":"test_2234","question":"when did the log flume closed at alton towers?","golden_answers":["10\u00a0October\u00a02015","2015"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: when did the log flume closed at alton towers?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["10\u00a0October\u00a02015","2015"]},"style":"rule"},"extra_info":{"index":498,"split":"test"}}
{"id":"test_147","question":"who has won india's next super star?","golden_answers":["Natasha Bharadwaj","Aman Gandotra"],"data_source":"nq","prompt":[{"content":"\nAnswer the given question. Every time you receive new information, you must first conduct reasoning inside <think> ... <\/think>. After reasoning, if you find you lack some knowledge, you can call a specialized LLM by writing a query inside <search> LLM-Name:Your-Query <\/search>.     + Important: You must replace LLM-Name with the exact name of a model selected from [Qwen2.5-7B-Instruct, LLaMA-3.1-8B-Instruct, LLaMA-3.1-70B-Instruct, Mistral-7B-Instruct, Mixtral-8x22B-Instruct, Gemma-2-27B-Instruct].     + You must replace Your-Query with the real query you want to ask.     + Never output the placeholder format <search> LLM-Name:Your-Query <\/search>. Always replace both parts correctly. Before each LLM call, you must explicitly reason inside <think> ... <\/think> about:     + Why external information is needed.     + Which LLM from the list is most suitable for answering your query, based on the brief model descriptions provided below (for reference). When you call an LLM, the response will be returned between <information> and <\/information>. You must not limit yourself to repeatedly calling a single LLM (unless its provided information is consistently the most effective and informative). You are encouraged to explore and utilize different LLMs to better understand their respective strengths and weaknesses. It is also acceptable\u2014and recommended\u2014to call different LLMs multiple times for the same input question to gather more comprehensive information. \n\n#### The Introduction of Each LLM \nQwen2.5-7B-Instruct:Qwen2.5-7B-Instruct is a powerful Chinese-English instruction-tuned large language model designed for tasks in language, coding, mathematics, and reasoning. As part of the Qwen2.5 series, it features enhanced knowledge, stronger coding and math abilities, improved instruction following, better handling of long and structured texts, and supports up to 128K context tokens. It also offers multilingual capabilities across over 29 languages.\n\nLLaMA-3.1-8B-Instruct:LLaMA-3.1-8B-Instruct is an 8-billion-parameter instruction-tuned language model optimized for multilingual dialogue. It provides strong language understanding, reasoning, and text generation performance, outperforming many open-source and closed-source models on standard industry benchmarks.\n\nLLaMA-3.1-70B-Instruct:LLaMA-3.1-70B-Instruct is a 70-billion-parameter state-of-the-art language model designed for advanced multilingual dialogue tasks. It excels in language comprehension, complex reasoning, and high-quality text generation, setting a new standard against both open and closed models in benchmark evaluations.\n\nMistral-7B-Instruct:Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B-v0.3 language model designed to follow instructions, complete user requests, and generate creative text. It was trained on diverse public conversation datasets to enhance its ability to handle interactive tasks effectively.\n\nMixtral-8x22B-Instruct:Mixtral-8x22B-Instruct is a cutting-edge sparse Mixture-of-Experts (SMoE) large language model from MistralAI. It efficiently uses 39B active parameters out of 141B total, delivering high performance at lower costs. The model excels at following instructions, completing tasks, and generating creative text, with strong skills in multiple languages (English, French, Italian, German, Spanish), mathematics, and coding. It also supports native function calling and handles long contexts up to 64K tokens for better information recall.\n\nGemma-2-27B-Instruct:Gemma-2-27B-Instruct is a cutting-edge, instruction-tuned text generation model developed by Google. Built using the same technology as Gemini, it excels at text understanding, transformation, and code generation. As a lightweight, decoder-only model with open weights, it is ideal for tasks like question answering, summarization, and reasoning. Its compact size enables deployment on laptops, desktops, or private cloud setups, making powerful AI more accessible.\n\nIf you find that no further external knowledge is needed, you can directly provide your final answer inside <answer> ... <\/answer>, without additional explanation or illustration. For example: <answer> Beijing <\/answer>.     + Important: You must not output the placeholder text \"<answer> and <\/answer>\" alone.     + You must insert your actual answer between <answer> and <\/answer>, following the correct format. Question: who has won india's next super star?\n\n","role":"user"}],"ability":"fact-reasoning","reward_model":{"ground_truth":{"target":["Natasha Bharadwaj","Aman Gandotra"]},"style":"rule"},"extra_info":{"index":499,"split":"test"}}
